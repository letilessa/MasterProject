{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5232761767002161625\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 711327744\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 6137285264501482943\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1\"\n",
      "]\n",
      "1.5.0 2.2.0\n"
     ]
    }
   ],
   "source": [
    "#YOLOv2 lightweight implementation based on: https://github.com/experiencor/keras-yolo2\n",
    "\n",
    "#Imports and GPU configuration:\n",
    "from keras import backend as K\n",
    "from keras import layers \n",
    "from keras import models \n",
    "from keras import optimizers\n",
    "from keras.utils import Sequence\n",
    "from keras.applications.mobilenet import MobileNet  \n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from keras.layers import Conv2D, SeparableConv2D, BatchNormalization, LeakyReLU, Input, Lambda, Reshape\n",
    "\n",
    "import os\n",
    "import io\n",
    "import PIL  \n",
    "import cv2\n",
    "import copy\n",
    "import time\n",
    "import h5py\n",
    "import keras\n",
    "import numpy as np\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import matplotlib.pyplot as plt \n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.9)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "print(tf.__version__, keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up variables:\n",
    "LABELS = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse',\n",
    "          'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "IMAGE_H, IMAGE_W = 416, 416\n",
    "GRID_H,  GRID_W  = 13 , 13\n",
    "BOX              = 5\n",
    "CLASS            = len(LABELS)\n",
    "CLASS_WEIGHTS    = np.ones(CLASS, dtype='float32')\n",
    "OBJ_THRESHOLD    = 0.3#0.5\n",
    "NMS_THRESHOLD    = 0.3#0.45\n",
    "ANCHORS          = [0.57273, 0.677385, 1.87446, 2.06253, 3.33843, 5.47434, 7.88282, 3.52778, 9.77052, 9.16828]\n",
    "\n",
    "NO_OBJECT_SCALE  = 0.5\n",
    "OBJECT_SCALE     = 5.0\n",
    "COORD_SCALE      = 1.0\n",
    "CLASS_SCALE      = 1.0\n",
    "\n",
    "BATCH_SIZE       = 24\n",
    "WARM_UP_BATCHES  = 3\n",
    "TRUE_BOX_BUFFER  = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting pre-trained convolutional base:\n",
    "conv_base=MobileNet(input_shape=(224,224,3), include_top=False, weights='imagenet')\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 416, 416, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mobilenet_1.00_224 (Model)      multiple             3228864     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_1 (SeparableCo (None, 13, 13, 1024) 1057792     mobilenet_1.00_224[1][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 13, 13, 1024) 4096        separable_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 13, 13, 1024) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_2 (SeparableCo (None, 13, 13, 1024) 1057792     leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 13, 13, 1024) 4096        separable_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 13, 13, 1024) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_3 (SeparableCo (None, 13, 13, 1024) 1057792     leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 13, 13, 1024) 4096        separable_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 13, 13, 1024) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 13, 13, 125)  128125      leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 13, 13, 5, 25 0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1, 1, 1, 50,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 13, 13, 5, 25 0           reshape_1[0][0]                  \n",
      "                                                                 input_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 6,542,653\n",
      "Trainable params: 6,514,621\n",
      "Non-trainable params: 28,032\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Defining the detection model:\n",
    "input_image = Input(shape=(IMAGE_H, IMAGE_W, 3))\n",
    "true_boxes  = Input(shape=(1, 1, 1, TRUE_BOX_BUFFER , 4))\n",
    "\n",
    "x = conv_base(input_image)\n",
    "\n",
    "x = SeparableConv2D(1024, (3, 3), padding='same', use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "x = SeparableConv2D(1024, (3, 3), padding='same', use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "x = SeparableConv2D(1024, (3, 3), padding='same', use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "x = Conv2D(BOX * (4 + 1 + CLASS), (1,1), padding='same', activation='linear')(x)\n",
    "output = Reshape((GRID_H, GRID_W, BOX, 4 + 1 + CLASS))(x)\n",
    "output = Lambda(lambda args: args[0])([output, true_boxes])\n",
    "\n",
    "conv_base.trainable = True\n",
    "\n",
    "model = models.Model([input_image, true_boxes], output)\n",
    "\n",
    "# Initializing the weights of the detection layer: \n",
    "layer = model.layers[-4]\n",
    "weights = layer.get_weights()\n",
    "\n",
    "new_kernel = np.random.normal(size=weights[0].shape)/(GRID_H*GRID_W)\n",
    "new_bias   = np.random.normal(size=weights[1].shape)/(GRID_H*GRID_W)\n",
    "\n",
    "layer.set_weights([new_kernel, new_bias])\n",
    "        \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining YOLO loss function:\n",
    "def custom_loss(y_true, y_pred):\n",
    "    mask_shape = tf.shape(y_true)[:4]\n",
    "    \n",
    "    cell_x = tf.to_float(tf.reshape(tf.tile(tf.range(GRID_W), [GRID_H]), (1, GRID_H, GRID_W, 1, 1)))\n",
    "    cell_y = tf.transpose(cell_x, (0,2,1,3,4))\n",
    "\n",
    "    cell_grid = tf.tile(tf.concat([cell_x,cell_y], -1), [BATCH_SIZE, 1, 1, 5, 1])\n",
    "    \n",
    "    coord_mask = tf.zeros(mask_shape)\n",
    "    conf_mask  = tf.zeros(mask_shape)\n",
    "    class_mask = tf.zeros(mask_shape)\n",
    "    \n",
    "    seen = tf.Variable(0.)\n",
    "    total_recall = tf.Variable(0.)\n",
    "    \n",
    "    \"\"\"\n",
    "    Adjust prediction\n",
    "    \"\"\"\n",
    "    ### adjust x and y      \n",
    "    pred_box_xy = tf.sigmoid(y_pred[..., :2]) + cell_grid\n",
    "    \n",
    "    ### adjust w and h\n",
    "    pred_box_wh = tf.exp(y_pred[..., 2:4]) * np.reshape(ANCHORS, [1,1,1,BOX,2])\n",
    "    \n",
    "    ### adjust confidence\n",
    "    pred_box_conf = tf.sigmoid(y_pred[..., 4])\n",
    "    \n",
    "    ### adjust class probabilities\n",
    "    pred_box_class = y_pred[..., 5:]\n",
    "    \n",
    "    \"\"\"\n",
    "    Adjust ground truth\n",
    "    \"\"\"\n",
    "    ### adjust x and y\n",
    "    true_box_xy = y_true[..., 0:2] # relative position to the containing cell\n",
    "    \n",
    "    ### adjust w and h\n",
    "    true_box_wh = y_true[..., 2:4] # number of cells accross, horizontally and vertically\n",
    "    \n",
    "    ### adjust confidence\n",
    "    true_wh_half = true_box_wh / 2.\n",
    "    true_mins    = true_box_xy - true_wh_half\n",
    "    true_maxes   = true_box_xy + true_wh_half\n",
    "    \n",
    "    pred_wh_half = pred_box_wh / 2.\n",
    "    pred_mins    = pred_box_xy - pred_wh_half\n",
    "    pred_maxes   = pred_box_xy + pred_wh_half       \n",
    "    \n",
    "    intersect_mins  = tf.maximum(pred_mins,  true_mins)\n",
    "    intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
    "    intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "    \n",
    "    true_areas = true_box_wh[..., 0] * true_box_wh[..., 1]\n",
    "    pred_areas = pred_box_wh[..., 0] * pred_box_wh[..., 1]\n",
    "\n",
    "    union_areas = pred_areas + true_areas - intersect_areas\n",
    "    iou_scores  = tf.truediv(intersect_areas, union_areas)\n",
    "    \n",
    "    true_box_conf = iou_scores * y_true[..., 4]\n",
    "    \n",
    "    ### adjust class probabilities\n",
    "    true_box_class = tf.argmax(y_true[..., 5:], -1)\n",
    "    \n",
    "    \"\"\"\n",
    "    Determine the masks\n",
    "    \"\"\"\n",
    "    ### coordinate mask: simply the position of the ground truth boxes (the predictors)\n",
    "    coord_mask = tf.expand_dims(y_true[..., 4], axis=-1) * COORD_SCALE\n",
    "    \n",
    "    ### confidence mask: penelize predictors + penalize boxes with low IOU\n",
    "    # penalize the confidence of the boxes, which have IOU with some ground truth box < 0.6\n",
    "    true_xy = true_boxes[..., 0:2]\n",
    "    true_wh = true_boxes[..., 2:4]\n",
    "    \n",
    "    true_wh_half = true_wh / 2.\n",
    "    true_mins    = true_xy - true_wh_half\n",
    "    true_maxes   = true_xy + true_wh_half\n",
    "    \n",
    "    pred_xy = tf.expand_dims(pred_box_xy, 4)\n",
    "    pred_wh = tf.expand_dims(pred_box_wh, 4)\n",
    "    \n",
    "    pred_wh_half = pred_wh / 2.\n",
    "    pred_mins    = pred_xy - pred_wh_half\n",
    "    pred_maxes   = pred_xy + pred_wh_half    \n",
    "    \n",
    "    intersect_mins  = tf.maximum(pred_mins,  true_mins)\n",
    "    intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
    "    intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "    \n",
    "    true_areas = true_wh[..., 0] * true_wh[..., 1]\n",
    "    pred_areas = pred_wh[..., 0] * pred_wh[..., 1]\n",
    "\n",
    "    union_areas = pred_areas + true_areas - intersect_areas\n",
    "    iou_scores  = tf.truediv(intersect_areas, union_areas)\n",
    "\n",
    "    best_ious = tf.reduce_max(iou_scores, axis=4)\n",
    "    conf_mask = conf_mask + tf.to_float(best_ious < 0.6) * (1 - y_true[..., 4]) * NO_OBJECT_SCALE\n",
    "    \n",
    "    # penalize the confidence of the boxes, which are reponsible for corresponding ground truth box\n",
    "    conf_mask = conf_mask + y_true[..., 4] * OBJECT_SCALE\n",
    "    \n",
    "    ### class mask: simply the position of the ground truth boxes (the predictors)\n",
    "    class_mask = y_true[..., 4] * tf.gather(CLASS_WEIGHTS, true_box_class) * CLASS_SCALE       \n",
    "    \n",
    "    \"\"\"\n",
    "    Warm-up training\n",
    "    \"\"\"\n",
    "    no_boxes_mask = tf.to_float(coord_mask < COORD_SCALE/2.)\n",
    "    seen = tf.assign_add(seen, 1.)\n",
    "    \n",
    "    true_box_xy, true_box_wh, coord_mask = tf.cond(tf.less(seen, WARM_UP_BATCHES), \n",
    "                          lambda: [true_box_xy + (0.5 + cell_grid) * no_boxes_mask, \n",
    "                                   true_box_wh + tf.ones_like(true_box_wh) * np.reshape(ANCHORS, [1,1,1,BOX,2]) * no_boxes_mask, \n",
    "                                   tf.ones_like(coord_mask)],\n",
    "                          lambda: [true_box_xy, \n",
    "                                   true_box_wh,\n",
    "                                   coord_mask])\n",
    "    \n",
    "    \"\"\"\n",
    "    Finalize the loss\n",
    "    \"\"\"\n",
    "    nb_coord_box = tf.reduce_sum(tf.to_float(coord_mask > 0.0))\n",
    "    nb_conf_box  = tf.reduce_sum(tf.to_float(conf_mask  > 0.0))\n",
    "    nb_class_box = tf.reduce_sum(tf.to_float(class_mask > 0.0))\n",
    "    \n",
    "    loss_xy    = tf.reduce_sum(tf.square(true_box_xy-pred_box_xy)     * coord_mask) / (nb_coord_box + 1e-6) / 2.\n",
    "    loss_wh    = tf.reduce_sum(tf.square(true_box_wh-pred_box_wh)     * coord_mask) / (nb_coord_box + 1e-6) / 2.\n",
    "    loss_conf  = tf.reduce_sum(tf.square(true_box_conf-pred_box_conf) * conf_mask)  / (nb_conf_box  + 1e-6) / 2.\n",
    "    loss_class = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=true_box_class, logits=pred_box_class)\n",
    "    loss_class = tf.reduce_sum(loss_class * class_mask) / (nb_class_box + 1e-6)\n",
    "    \n",
    "    loss = loss_xy + loss_wh + loss_conf + loss_class\n",
    "    \n",
    "    nb_true_box = tf.reduce_sum(y_true[..., 4])\n",
    "    nb_pred_box = tf.reduce_sum(tf.to_float(true_box_conf > 0.5) * tf.to_float(pred_box_conf > 0.3))\n",
    "\n",
    "    \"\"\"\n",
    "    Debugging code\n",
    "    \"\"\"    \n",
    "    current_recall = nb_pred_box/(nb_true_box + 1e-6)\n",
    "    total_recall = tf.assign_add(total_recall, current_recall) \n",
    "\n",
    "    loss = tf.Print(loss, [tf.zeros((1))], message='Dummy Line \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss_xy], message='Loss XY \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss_wh], message='Loss WH \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss_conf], message='Loss Conf \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss_class], message='Loss Class \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss], message='Total Loss \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [current_recall], message='Current Recall \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [total_recall/seen], message='Average Recall \\t', summarize=1000)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_config = {\n",
    "    'IMAGE_H'         : IMAGE_H, \n",
    "    'IMAGE_W'         : IMAGE_W,\n",
    "    'GRID_H'          : GRID_H,  \n",
    "    'GRID_W'          : GRID_W,\n",
    "    'BOX'             : BOX,\n",
    "    'LABELS'          : LABELS,\n",
    "    'CLASS'           : len(LABELS),\n",
    "    'ANCHORS'         : ANCHORS,\n",
    "    'BATCH_SIZE'      : BATCH_SIZE,\n",
    "    'TRUE_BOX_BUFFER' : 50,\n",
    "}\n",
    "\n",
    "#MobileNet normalization function:\n",
    "def normalize(image):\n",
    "    image = image / 255.\n",
    "    image = image - 0.5\n",
    "    image = image * 2.\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5011\n",
      "5717\n",
      "10728\n",
      "5823\n",
      "447\n",
      "243\n"
     ]
    }
   ],
   "source": [
    "#Parsing annotations to construct train generator and validation generator:\n",
    "voc_path ='/media/eHD/datasets/pascalVOC2012/VOCtrain/VOC2012/'\n",
    "train_ids = get_ids(voc_path, train_set)\n",
    "val_ids = get_ids(voc_path, val_set)\n",
    "train_ids_2007 = get_ids('/media/eHD/datasets/pascalVOC2007/VOC_train/', sets_from_2007)\n",
    "\n",
    "train_imgs_2007, seen_train_labels_2007 = parse_annotation('/media/eHD/datasets/pascalVOC2007/VOC_train/', \n",
    "                                                           '/media/eHD/datasets/pascalVOC2007/VOC_train/JPEGImages/', \n",
    "                                                           train_ids_2007, labels=LABELS)\n",
    "train_imgs_2012, seen_train_labels_2012 = parse_annotation(voc_path, '/media/eHD/datasets/pascalVOC2012/VOCtrain/VOC2012/JPEGImages/', \n",
    "                                                           train_ids, labels=LABELS)\n",
    "train_imgs=train_imgs_2007+train_imgs_2012\n",
    "valid_imgs, seen_valid_labels = parse_annotation(voc_path, '/media/eHD/datasets/pascalVOC2012/VOCtrain/VOC2012/JPEGImages/', \n",
    "                                                 val_ids, labels=LABELS)\n",
    "\n",
    "train_batch = BatchGenerator(train_imgs, generator_config, norm=normalize)\n",
    "valid_batch = BatchGenerator(valid_imgs, generator_config, norm=normalize, jitter=False)\n",
    "\n",
    "print(len(train_imgs_2007))\n",
    "print(len(train_imgs_2012))\n",
    "print(len(train_imgs))\n",
    "print(len(valid_imgs))\n",
    "print(len(train_batch))\n",
    "print(len(valid_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "447/447 [==============================] - 323s 722ms/step - loss: 5.4419 - val_loss: 4.9341\n",
      "Epoch 2/100\n",
      "447/447 [==============================] - 317s 708ms/step - loss: 4.7359 - val_loss: 4.5649\n",
      "Epoch 3/100\n",
      "447/447 [==============================] - 317s 710ms/step - loss: 4.4326 - val_loss: 4.4415\n",
      "Epoch 4/100\n",
      "447/447 [==============================] - 319s 713ms/step - loss: 4.2575 - val_loss: 4.2258\n",
      "Epoch 5/100\n",
      "447/447 [==============================] - 318s 712ms/step - loss: 4.1459 - val_loss: 4.0868\n",
      "\n",
      "Epoch 00005: saving model to /media/eHD/leticia/models/test12.h5\n",
      "Epoch 6/100\n",
      "447/447 [==============================] - 315s 705ms/step - loss: 4.0170 - val_loss: 4.0116\n",
      "Epoch 7/100\n",
      "447/447 [==============================] - 319s 713ms/step - loss: 3.9506 - val_loss: 3.9929\n",
      "Epoch 8/100\n",
      "447/447 [==============================] - 316s 706ms/step - loss: 3.8752 - val_loss: 3.8875\n",
      "Epoch 9/100\n",
      "447/447 [==============================] - 318s 711ms/step - loss: 3.8250 - val_loss: 3.8534\n",
      "Epoch 10/100\n",
      "447/447 [==============================] - 319s 714ms/step - loss: 3.7555 - val_loss: 3.8523\n",
      "\n",
      "Epoch 00010: saving model to /media/eHD/leticia/models/test12.h5\n",
      "Epoch 11/100\n",
      "447/447 [==============================] - 316s 708ms/step - loss: 3.7153 - val_loss: 3.7517\n",
      "Epoch 12/100\n",
      "447/447 [==============================] - 316s 706ms/step - loss: 3.6583 - val_loss: 3.7305\n",
      "Epoch 13/100\n",
      "447/447 [==============================] - 316s 708ms/step - loss: 3.6154 - val_loss: 3.6263\n",
      "Epoch 14/100\n",
      "447/447 [==============================] - 317s 709ms/step - loss: 3.5941 - val_loss: 3.6200\n",
      "Epoch 15/100\n",
      "447/447 [==============================] - 318s 711ms/step - loss: 3.5432 - val_loss: 3.6367\n",
      "\n",
      "Epoch 00015: saving model to /media/eHD/leticia/models/test12.h5\n",
      "Epoch 16/100\n",
      "447/447 [==============================] - 316s 706ms/step - loss: 3.5094 - val_loss: 3.5982\n",
      "Epoch 17/100\n",
      "447/447 [==============================] - 317s 709ms/step - loss: 3.4934 - val_loss: 3.5535\n",
      "Epoch 18/100\n",
      "447/447 [==============================] - 318s 711ms/step - loss: 3.4655 - val_loss: 3.5658\n",
      "Epoch 19/100\n",
      "447/447 [==============================] - 316s 708ms/step - loss: 3.4395 - val_loss: 3.5216\n",
      "Epoch 20/100\n",
      "447/447 [==============================] - 314s 702ms/step - loss: 3.4043 - val_loss: 3.4657\n",
      "\n",
      "Epoch 00020: saving model to /media/eHD/leticia/models/test12.h5\n",
      "Epoch 21/100\n",
      "447/447 [==============================] - 317s 709ms/step - loss: 3.4055 - val_loss: 3.4521\n",
      "Epoch 22/100\n",
      "447/447 [==============================] - 314s 703ms/step - loss: 3.3663 - val_loss: 3.4306\n",
      "Epoch 23/100\n",
      "447/447 [==============================] - 316s 706ms/step - loss: 3.3585 - val_loss: 3.4009\n",
      "Epoch 24/100\n",
      "447/447 [==============================] - 315s 705ms/step - loss: 3.3270 - val_loss: 3.3902\n",
      "Epoch 25/100\n",
      "447/447 [==============================] - 317s 709ms/step - loss: 3.3079 - val_loss: 3.3684\n",
      "\n",
      "Epoch 00025: saving model to /media/eHD/leticia/models/test12.h5\n",
      "Epoch 26/100\n",
      "447/447 [==============================] - 318s 711ms/step - loss: 3.3178 - val_loss: 3.3880\n",
      "Epoch 27/100\n",
      "447/447 [==============================] - 317s 709ms/step - loss: 3.2717 - val_loss: 3.3955\n",
      "Epoch 28/100\n",
      "447/447 [==============================] - 320s 715ms/step - loss: 3.2803 - val_loss: 3.3965\n",
      "Epoch 29/100\n",
      "447/447 [==============================] - 315s 704ms/step - loss: 3.2483 - val_loss: 3.2911\n",
      "Epoch 30/100\n",
      "447/447 [==============================] - 315s 705ms/step - loss: 3.2492 - val_loss: 3.3084\n",
      "\n",
      "Epoch 00030: saving model to /media/eHD/leticia/models/test12.h5\n",
      "Epoch 31/100\n",
      "447/447 [==============================] - 320s 717ms/step - loss: 3.2339 - val_loss: 3.3060\n",
      "Epoch 32/100\n",
      "447/447 [==============================] - 316s 706ms/step - loss: 3.1955 - val_loss: 3.2609\n",
      "Epoch 33/100\n",
      "447/447 [==============================] - 316s 706ms/step - loss: 3.1943 - val_loss: 3.2569\n",
      "Epoch 34/100\n",
      "447/447 [==============================] - 316s 706ms/step - loss: 3.1700 - val_loss: 3.2725\n",
      "Epoch 35/100\n",
      "447/447 [==============================] - 316s 706ms/step - loss: 3.1608 - val_loss: 3.2522\n",
      "\n",
      "Epoch 00035: saving model to /media/eHD/leticia/models/test12.h5\n",
      "Epoch 36/100\n",
      "447/447 [==============================] - 315s 704ms/step - loss: 3.1780 - val_loss: 3.2171\n",
      "Epoch 37/100\n",
      "447/447 [==============================] - 321s 717ms/step - loss: 3.1366 - val_loss: 3.2349\n",
      "Epoch 38/100\n",
      "447/447 [==============================] - 329s 736ms/step - loss: 3.1398 - val_loss: 3.1891\n",
      "Epoch 39/100\n",
      "447/447 [==============================] - 332s 743ms/step - loss: 3.1190 - val_loss: 3.2296\n",
      "Epoch 40/100\n",
      "447/447 [==============================] - 327s 733ms/step - loss: 3.1067 - val_loss: 3.1812\n",
      "\n",
      "Epoch 00040: saving model to /media/eHD/leticia/models/test12.h5\n",
      "Epoch 41/100\n",
      "447/447 [==============================] - 329s 737ms/step - loss: 3.1076 - val_loss: 3.1974\n",
      "Epoch 42/100\n",
      "447/447 [==============================] - 328s 733ms/step - loss: 3.1022 - val_loss: 3.1599\n",
      "Epoch 43/100\n",
      "447/447 [==============================] - 329s 737ms/step - loss: 3.0977 - val_loss: 3.1578\n",
      "Epoch 44/100\n",
      "447/447 [==============================] - 330s 738ms/step - loss: 3.0622 - val_loss: 3.1316\n",
      "Epoch 45/100\n",
      "447/447 [==============================] - 326s 730ms/step - loss: 3.0494 - val_loss: 3.1198\n",
      "\n",
      "Epoch 00045: saving model to /media/eHD/leticia/models/test12.h5\n",
      "Epoch 46/100\n",
      "447/447 [==============================] - 330s 739ms/step - loss: 3.0482 - val_loss: 3.1494\n",
      "Epoch 47/100\n",
      "447/447 [==============================] - 330s 738ms/step - loss: 3.0566 - val_loss: 3.1332\n",
      "Epoch 48/100\n",
      "447/447 [==============================] - 328s 734ms/step - loss: 3.0428 - val_loss: 3.1670\n",
      "Epoch 49/100\n",
      "447/447 [==============================] - 333s 746ms/step - loss: 3.0323 - val_loss: 3.1466\n",
      "Epoch 50/100\n",
      "447/447 [==============================] - 328s 735ms/step - loss: 3.0225 - val_loss: 3.0992\n",
      "\n",
      "Epoch 00050: saving model to /media/eHD/leticia/models/test12.h5\n",
      "Epoch 51/100\n",
      "447/447 [==============================] - 330s 738ms/step - loss: 3.0006 - val_loss: 3.0943\n",
      "Epoch 52/100\n",
      "447/447 [==============================] - 331s 741ms/step - loss: 3.0192 - val_loss: 3.0712\n",
      "Epoch 53/100\n",
      "447/447 [==============================] - 332s 742ms/step - loss: 3.0104 - val_loss: 3.0976\n",
      "Epoch 54/100\n",
      "447/447 [==============================] - 329s 737ms/step - loss: 2.9895 - val_loss: 3.0623\n",
      "Epoch 55/100\n",
      "447/447 [==============================] - 331s 741ms/step - loss: 3.0246 - val_loss: 3.0783\n",
      "\n",
      "Epoch 00055: saving model to /media/eHD/leticia/models/test12.h5\n",
      "Epoch 56/100\n",
      "447/447 [==============================] - 330s 738ms/step - loss: 2.9872 - val_loss: 3.0919\n",
      "Epoch 57/100\n",
      "447/447 [==============================] - 330s 738ms/step - loss: 2.9841 - val_loss: 3.0738\n",
      "Epoch 58/100\n",
      "447/447 [==============================] - 331s 741ms/step - loss: 2.9686 - val_loss: 3.0238\n",
      "Epoch 59/100\n",
      "447/447 [==============================] - 327s 732ms/step - loss: 2.9714 - val_loss: 3.0774\n",
      "Epoch 60/100\n",
      "447/447 [==============================] - 332s 742ms/step - loss: 2.9532 - val_loss: 3.0505\n",
      "\n",
      "Epoch 00060: saving model to /media/eHD/leticia/models/test12.h5\n",
      "Epoch 61/100\n",
      "447/447 [==============================] - 330s 738ms/step - loss: 2.9437 - val_loss: 3.0473\n",
      "Epoch 62/100\n",
      "447/447 [==============================] - 332s 742ms/step - loss: 2.9625 - val_loss: 3.0253\n",
      "Epoch 63/100\n",
      "447/447 [==============================] - 334s 747ms/step - loss: 2.9568 - val_loss: 3.0213\n",
      "Epoch 64/100\n",
      "447/447 [==============================] - 331s 741ms/step - loss: 2.9329 - val_loss: 3.0273\n",
      "Epoch 65/100\n",
      "447/447 [==============================] - 330s 738ms/step - loss: 2.9317 - val_loss: 3.0026\n",
      "\n",
      "Epoch 00065: saving model to /media/eHD/leticia/models/test12.h5\n",
      "Epoch 66/100\n",
      "447/447 [==============================] - 331s 741ms/step - loss: 2.9241 - val_loss: 3.0183\n",
      "Epoch 67/100\n",
      "447/447 [==============================] - 329s 737ms/step - loss: 2.9204 - val_loss: 2.9987\n",
      "Epoch 68/100\n",
      "447/447 [==============================] - 330s 739ms/step - loss: 2.9205 - val_loss: 2.9916\n",
      "Epoch 69/100\n",
      "447/447 [==============================] - 332s 742ms/step - loss: 2.9191 - val_loss: 2.9948\n",
      "Epoch 70/100\n",
      "447/447 [==============================] - 331s 740ms/step - loss: 2.8889 - val_loss: 2.9898\n",
      "\n",
      "Epoch 00070: saving model to /media/eHD/leticia/models/test12.h5\n",
      "Epoch 71/100\n",
      "447/447 [==============================] - 327s 732ms/step - loss: 2.9034 - val_loss: 2.9648\n",
      "Epoch 72/100\n",
      "447/447 [==============================] - 332s 743ms/step - loss: 2.9085 - val_loss: 2.9956\n",
      "Epoch 73/100\n",
      "447/447 [==============================] - 330s 739ms/step - loss: 2.9020 - val_loss: 2.9587\n",
      "Epoch 74/100\n",
      "447/447 [==============================] - 330s 739ms/step - loss: 2.9036 - val_loss: 2.9650\n",
      "Epoch 75/100\n",
      "447/447 [==============================] - 327s 732ms/step - loss: 2.8931 - val_loss: 2.9689\n",
      "\n",
      "Epoch 00075: saving model to /media/eHD/leticia/models/test12.h5\n",
      "Epoch 76/100\n",
      "447/447 [==============================] - 328s 734ms/step - loss: 2.8874 - val_loss: 2.9586\n",
      "Epoch 77/100\n",
      "447/447 [==============================] - 331s 739ms/step - loss: 2.8679 - val_loss: 2.9592\n",
      "Epoch 78/100\n",
      "447/447 [==============================] - 331s 740ms/step - loss: 2.8603 - val_loss: 2.9698\n",
      "Epoch 79/100\n",
      "447/447 [==============================] - 331s 741ms/step - loss: 2.8668 - val_loss: 2.9522\n",
      "Epoch 80/100\n",
      "447/447 [==============================] - 331s 741ms/step - loss: 2.8522 - val_loss: 2.9681\n",
      "\n",
      "Epoch 00080: saving model to /media/eHD/leticia/models/test12.h5\n",
      "Epoch 81/100\n",
      "447/447 [==============================] - 331s 740ms/step - loss: 2.8611 - val_loss: 2.9575\n",
      "Epoch 82/100\n",
      "447/447 [==============================] - 330s 739ms/step - loss: 2.8512 - val_loss: 2.9268\n",
      "Epoch 83/100\n",
      "447/447 [==============================] - 330s 739ms/step - loss: 2.8610 - val_loss: 2.9523\n",
      "Epoch 84/100\n",
      "447/447 [==============================] - 331s 741ms/step - loss: 2.8422 - val_loss: 2.9250\n",
      "Epoch 85/100\n",
      "447/447 [==============================] - 330s 739ms/step - loss: 2.8448 - val_loss: 2.9201\n",
      "\n",
      "Epoch 00085: saving model to /media/eHD/leticia/models/test12.h5\n",
      "Epoch 86/100\n",
      "447/447 [==============================] - 329s 736ms/step - loss: 2.8337 - val_loss: 2.9409\n",
      "Epoch 87/100\n",
      "447/447 [==============================] - 330s 737ms/step - loss: 2.8338 - val_loss: 2.9294\n",
      "Epoch 88/100\n",
      "447/447 [==============================] - 332s 742ms/step - loss: 2.8424 - val_loss: 2.9142\n",
      "Epoch 89/100\n",
      "447/447 [==============================] - 332s 742ms/step - loss: 2.8297 - val_loss: 2.9079\n",
      "Epoch 90/100\n",
      "447/447 [==============================] - 329s 735ms/step - loss: 2.8282 - val_loss: 2.9112\n",
      "\n",
      "Epoch 00090: saving model to /media/eHD/leticia/models/test12.h5\n",
      "Epoch 91/100\n",
      "447/447 [==============================] - 329s 735ms/step - loss: 2.8299 - val_loss: 2.9061\n",
      "Epoch 92/100\n",
      "447/447 [==============================] - 330s 737ms/step - loss: 2.8226 - val_loss: 2.9022\n",
      "Epoch 93/100\n",
      "447/447 [==============================] - 335s 748ms/step - loss: 2.8135 - val_loss: 2.8959\n",
      "Epoch 94/100\n",
      "447/447 [==============================] - 334s 746ms/step - loss: 2.8404 - val_loss: 2.9200\n",
      "Epoch 95/100\n",
      "447/447 [==============================] - 331s 739ms/step - loss: 2.8165 - val_loss: 2.9205\n",
      "\n",
      "Epoch 00095: saving model to /media/eHD/leticia/models/test12.h5\n",
      "Epoch 96/100\n",
      "447/447 [==============================] - 327s 731ms/step - loss: 2.8107 - val_loss: 2.9041\n",
      "Epoch 97/100\n",
      "447/447 [==============================] - 330s 739ms/step - loss: 2.8086 - val_loss: 2.8895\n",
      "Epoch 98/100\n",
      "447/447 [==============================] - 330s 738ms/step - loss: 2.8058 - val_loss: 2.8840\n",
      "Epoch 99/100\n",
      "447/447 [==============================] - 330s 738ms/step - loss: 2.7940 - val_loss: 2.8862\n",
      "Epoch 100/100\n",
      "447/447 [==============================] - 330s 738ms/step - loss: 2.8039 - val_loss: 2.8941\n",
      "\n",
      "Epoch 00100: saving model to /media/eHD/leticia/models/test12.h5\n"
     ]
    }
   ],
   "source": [
    "#Setting callbacks and training:\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                              patience=5, min_lr=1e-7)\n",
    "\n",
    "checkpoint = ModelCheckpoint('/media/eHD/leticia/models/test13.{epoch:02d}-{val_loss:.2f}.h5', \n",
    "                             monitor='val_loss', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='min', \n",
    "                             period=5)\n",
    "\n",
    "tb_counter  = len([log for log in os.listdir('/media/eHD/leticia/logs/') if 'voc_' in log]) + 1\n",
    "tensorboard = TensorBoard(log_dir=os.path.expanduser('~/logs/') + 'voc_' + '_' + str(tb_counter), \n",
    "                          histogram_freq=0, \n",
    "                          write_graph=True, \n",
    "                          write_images=False)\n",
    " \n",
    "sgd = optimizers.SGD(lr=1e-4, decay=0.0005, momentum=0.9)\n",
    "model.compile(optimizer=sgd, loss=custom_loss) \n",
    "\n",
    "history=model.fit_generator(generator= train_batch, \n",
    "                    steps_per_epoch  = len(train_batch), \n",
    "                    epochs           = 100, \n",
    "                    verbose          = 1,\n",
    "                    validation_data  = valid_batch,\n",
    "                    validation_steps = len(valid_batch),\n",
    "                    callbacks        = [checkpoint, tensorboard, reduce_lr])\n",
    "\n",
    "model.save('/media/eHD/leticia/models/test13.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test13\n",
    "plt.plot(history.history['loss']) \n",
    "plt.plot(history.history['val_loss']) \n",
    "plt.title('model loss') \n",
    "plt.ylabel('loss') \n",
    "plt.xlabel('epoch') \n",
    "plt.legend(['train', 'val'], loc='upper left') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "447/447 [==============================] - 326s 729ms/step - loss: 5.2692 - val_loss: 4.7884\n",
      "Epoch 2/100\n",
      "447/447 [==============================] - 320s 715ms/step - loss: 4.3819 - val_loss: 4.2030\n",
      "Epoch 3/100\n",
      "447/447 [==============================] - 320s 715ms/step - loss: 4.0399 - val_loss: 4.0299\n",
      "Epoch 4/100\n",
      "447/447 [==============================] - 322s 720ms/step - loss: 3.8034 - val_loss: 3.7156\n",
      "Epoch 5/100\n",
      "447/447 [==============================] - 322s 721ms/step - loss: 3.6688 - val_loss: 3.6090\n",
      "\n",
      "Epoch 00005: val_loss improved from inf to 3.60899, saving model to /media/eHD/leticia/models/test13.05-3.61.h5\n",
      "Epoch 6/100\n",
      "447/447 [==============================] - 320s 715ms/step - loss: 3.5201 - val_loss: 3.5038\n",
      "Epoch 7/100\n",
      "447/447 [==============================] - 324s 725ms/step - loss: 3.4219 - val_loss: 3.4496\n",
      "Epoch 8/100\n",
      "447/447 [==============================] - 321s 718ms/step - loss: 3.3418 - val_loss: 3.3576\n",
      "Epoch 9/100\n",
      "447/447 [==============================] - 321s 717ms/step - loss: 3.2710 - val_loss: 3.2907\n",
      "Epoch 10/100\n",
      "447/447 [==============================] - 324s 724ms/step - loss: 3.2139 - val_loss: 3.2478\n",
      "\n",
      "Epoch 00010: val_loss improved from 3.60899 to 3.24775, saving model to /media/eHD/leticia/models/test13.10-3.25.h5\n",
      "Epoch 11/100\n",
      "447/447 [==============================] - 323s 722ms/step - loss: 3.1622 - val_loss: 3.1836\n",
      "Epoch 12/100\n",
      "447/447 [==============================] - 320s 717ms/step - loss: 3.1040 - val_loss: 3.1202\n",
      "Epoch 13/100\n",
      "447/447 [==============================] - 319s 714ms/step - loss: 3.0305 - val_loss: 3.0708\n",
      "Epoch 14/100\n",
      "447/447 [==============================] - 320s 717ms/step - loss: 2.9989 - val_loss: 3.0094\n",
      "Epoch 15/100\n",
      "447/447 [==============================] - 321s 719ms/step - loss: 2.9434 - val_loss: 2.9747\n",
      "\n",
      "Epoch 00015: val_loss improved from 3.24775 to 2.97468, saving model to /media/eHD/leticia/models/test13.15-2.97.h5\n",
      "Epoch 16/100\n",
      "447/447 [==============================] - 319s 714ms/step - loss: 2.9318 - val_loss: 2.9598\n",
      "Epoch 17/100\n",
      "447/447 [==============================] - 324s 725ms/step - loss: 2.8772 - val_loss: 2.9064\n",
      "Epoch 18/100\n",
      "447/447 [==============================] - 323s 723ms/step - loss: 2.8307 - val_loss: 2.8778\n",
      "Epoch 19/100\n",
      "447/447 [==============================] - 322s 720ms/step - loss: 2.8237 - val_loss: 2.8396\n",
      "Epoch 20/100\n",
      "447/447 [==============================] - 319s 713ms/step - loss: 2.7950 - val_loss: 2.8091\n",
      "\n",
      "Epoch 00020: val_loss improved from 2.97468 to 2.80912, saving model to /media/eHD/leticia/models/test13.20-2.81.h5\n",
      "Epoch 21/100\n",
      "447/447 [==============================] - 320s 717ms/step - loss: 2.7582 - val_loss: 2.7903\n",
      "Epoch 22/100\n",
      "447/447 [==============================] - 322s 720ms/step - loss: 2.7276 - val_loss: 2.7610\n",
      "Epoch 23/100\n",
      "447/447 [==============================] - 319s 714ms/step - loss: 2.6944 - val_loss: 2.7392\n",
      "Epoch 24/100\n",
      "447/447 [==============================] - 321s 717ms/step - loss: 2.6743 - val_loss: 2.7247\n",
      "Epoch 25/100\n",
      "447/447 [==============================] - 322s 721ms/step - loss: 2.6517 - val_loss: 2.6862\n",
      "\n",
      "Epoch 00025: val_loss improved from 2.80912 to 2.68618, saving model to /media/eHD/leticia/models/test13.25-2.69.h5\n",
      "Epoch 26/100\n",
      "447/447 [==============================] - 326s 730ms/step - loss: 2.6447 - val_loss: 2.6773\n",
      "Epoch 27/100\n",
      "447/447 [==============================] - 323s 724ms/step - loss: 2.6017 - val_loss: 2.6638\n",
      "Epoch 28/100\n",
      "447/447 [==============================] - 320s 715ms/step - loss: 2.5783 - val_loss: 2.6189\n",
      "Epoch 29/100\n",
      "447/447 [==============================] - 319s 714ms/step - loss: 2.5694 - val_loss: 2.6278\n",
      "Epoch 30/100\n",
      "447/447 [==============================] - 320s 717ms/step - loss: 2.5499 - val_loss: 2.5848\n",
      "\n",
      "Epoch 00030: val_loss improved from 2.68618 to 2.58478, saving model to /media/eHD/leticia/models/test13.30-2.58.h5\n",
      "Epoch 31/100\n",
      "447/447 [==============================] - 324s 725ms/step - loss: 2.5476 - val_loss: 2.6024\n",
      "Epoch 32/100\n",
      "447/447 [==============================] - 321s 719ms/step - loss: 2.5333 - val_loss: 2.5774\n",
      "Epoch 33/100\n",
      "447/447 [==============================] - 322s 721ms/step - loss: 2.5021 - val_loss: 2.5512\n",
      "Epoch 34/100\n",
      "447/447 [==============================] - 323s 722ms/step - loss: 2.4871 - val_loss: 2.5300\n",
      "Epoch 35/100\n",
      "447/447 [==============================] - 324s 724ms/step - loss: 2.4786 - val_loss: 2.5368\n",
      "\n",
      "Epoch 00035: val_loss improved from 2.58478 to 2.53676, saving model to /media/eHD/leticia/models/test13.35-2.54.h5\n",
      "Epoch 36/100\n",
      "447/447 [==============================] - 324s 725ms/step - loss: 2.4665 - val_loss: 2.5190\n",
      "Epoch 37/100\n",
      "447/447 [==============================] - 324s 724ms/step - loss: 2.4386 - val_loss: 2.5209\n",
      "Epoch 38/100\n",
      "447/447 [==============================] - 325s 726ms/step - loss: 2.4484 - val_loss: 2.5036\n",
      "Epoch 39/100\n",
      "447/447 [==============================] - 325s 727ms/step - loss: 2.4373 - val_loss: 2.4985\n",
      "Epoch 40/100\n",
      "447/447 [==============================] - 322s 721ms/step - loss: 2.4158 - val_loss: 2.4702\n",
      "\n",
      "Epoch 00040: val_loss improved from 2.53676 to 2.47020, saving model to /media/eHD/leticia/models/test13.40-2.47.h5\n",
      "Epoch 41/100\n",
      "447/447 [==============================] - 325s 727ms/step - loss: 2.4004 - val_loss: 2.4631\n",
      "Epoch 42/100\n",
      "447/447 [==============================] - 322s 721ms/step - loss: 2.4068 - val_loss: 2.4505\n",
      "Epoch 43/100\n",
      "447/447 [==============================] - 324s 725ms/step - loss: 2.3843 - val_loss: 2.4387\n",
      "Epoch 44/100\n",
      "447/447 [==============================] - 322s 721ms/step - loss: 2.3788 - val_loss: 2.4265\n",
      "Epoch 45/100\n",
      "447/447 [==============================] - 321s 719ms/step - loss: 2.3528 - val_loss: 2.4428\n",
      "\n",
      "Epoch 00045: val_loss improved from 2.47020 to 2.44279, saving model to /media/eHD/leticia/models/test13.45-2.44.h5\n",
      "Epoch 46/100\n",
      "447/447 [==============================] - 322s 719ms/step - loss: 2.3695 - val_loss: 2.4218\n",
      "Epoch 47/100\n",
      "447/447 [==============================] - 323s 723ms/step - loss: 2.3503 - val_loss: 2.4078\n",
      "Epoch 48/100\n",
      "447/447 [==============================] - 322s 721ms/step - loss: 2.3306 - val_loss: 2.3966\n",
      "Epoch 49/100\n",
      "447/447 [==============================] - 325s 728ms/step - loss: 2.3312 - val_loss: 2.3938\n",
      "Epoch 50/100\n",
      "447/447 [==============================] - 321s 718ms/step - loss: 2.3341 - val_loss: 2.4057\n",
      "\n",
      "Epoch 00050: val_loss improved from 2.44279 to 2.40574, saving model to /media/eHD/leticia/models/test13.50-2.41.h5\n",
      "Epoch 51/100\n",
      "447/447 [==============================] - 321s 717ms/step - loss: 2.3148 - val_loss: 2.3679\n",
      "Epoch 52/100\n",
      "447/447 [==============================] - 322s 721ms/step - loss: 2.3078 - val_loss: 2.3589\n",
      "Epoch 53/100\n",
      "447/447 [==============================] - 323s 723ms/step - loss: 2.3048 - val_loss: 2.3607\n",
      "Epoch 54/100\n",
      "447/447 [==============================] - 322s 719ms/step - loss: 2.2958 - val_loss: 2.3568\n",
      "Epoch 55/100\n",
      "447/447 [==============================] - 324s 725ms/step - loss: 2.2912 - val_loss: 2.3415\n",
      "\n",
      "Epoch 00055: val_loss improved from 2.40574 to 2.34151, saving model to /media/eHD/leticia/models/test13.55-2.34.h5\n",
      "Epoch 56/100\n",
      "447/447 [==============================] - 323s 722ms/step - loss: 2.2773 - val_loss: 2.3401\n",
      "Epoch 57/100\n",
      "447/447 [==============================] - 322s 721ms/step - loss: 2.2607 - val_loss: 2.3354\n",
      "Epoch 58/100\n",
      "447/447 [==============================] - 323s 723ms/step - loss: 2.2519 - val_loss: 2.3507\n",
      "Epoch 59/100\n",
      "447/447 [==============================] - 319s 714ms/step - loss: 2.2572 - val_loss: 2.3206\n",
      "Epoch 60/100\n",
      "447/447 [==============================] - 322s 721ms/step - loss: 2.2501 - val_loss: 2.3184\n",
      "\n",
      "Epoch 00060: val_loss improved from 2.34151 to 2.31839, saving model to /media/eHD/leticia/models/test13.60-2.32.h5\n",
      "Epoch 61/100\n",
      "447/447 [==============================] - 322s 720ms/step - loss: 2.2382 - val_loss: 2.3117\n",
      "Epoch 62/100\n",
      "447/447 [==============================] - 323s 722ms/step - loss: 2.2440 - val_loss: 2.2997\n",
      "Epoch 63/100\n",
      "447/447 [==============================] - 325s 727ms/step - loss: 2.2332 - val_loss: 2.2910\n",
      "Epoch 64/100\n",
      "447/447 [==============================] - 322s 721ms/step - loss: 2.2275 - val_loss: 2.2932\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447/447 [==============================] - 323s 723ms/step - loss: 2.2223 - val_loss: 2.2892\n",
      "\n",
      "Epoch 00065: val_loss improved from 2.31839 to 2.28919, saving model to /media/eHD/leticia/models/test13.65-2.29.h5\n",
      "Epoch 66/100\n",
      "447/447 [==============================] - 322s 720ms/step - loss: 2.2006 - val_loss: 2.2849\n",
      "Epoch 67/100\n",
      "447/447 [==============================] - 320s 716ms/step - loss: 2.1957 - val_loss: 2.2696\n",
      "Epoch 68/100\n",
      "447/447 [==============================] - 319s 714ms/step - loss: 2.2075 - val_loss: 2.2701\n",
      "Epoch 69/100\n",
      "447/447 [==============================] - 322s 720ms/step - loss: 2.2092 - val_loss: 2.2637\n",
      "Epoch 70/100\n",
      "447/447 [==============================] - 321s 718ms/step - loss: 2.1971 - val_loss: 2.2601\n",
      "\n",
      "Epoch 00070: val_loss improved from 2.28919 to 2.26006, saving model to /media/eHD/leticia/models/test13.70-2.26.h5\n",
      "Epoch 71/100\n",
      "447/447 [==============================] - 320s 717ms/step - loss: 2.2020 - val_loss: 2.2598\n",
      "Epoch 72/100\n",
      "447/447 [==============================] - 322s 720ms/step - loss: 2.1857 - val_loss: 2.2704\n",
      "Epoch 73/100\n",
      "447/447 [==============================] - 320s 715ms/step - loss: 2.1642 - val_loss: 2.2511\n",
      "Epoch 74/100\n",
      "447/447 [==============================] - 322s 721ms/step - loss: 2.1646 - val_loss: 2.2441\n",
      "Epoch 75/100\n",
      "447/447 [==============================] - 319s 713ms/step - loss: 2.1680 - val_loss: 2.2523\n",
      "\n",
      "Epoch 00075: val_loss improved from 2.26006 to 2.25231, saving model to /media/eHD/leticia/models/test13.75-2.25.h5\n",
      "Epoch 76/100\n",
      "447/447 [==============================] - 322s 720ms/step - loss: 2.1581 - val_loss: 2.2355\n",
      "Epoch 77/100\n",
      "431/447 [===========================>..] - ETA: 9s - loss: 2.1542 "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                              patience=5, min_lr=1e-7)\n",
    "\n",
    "checkpoint = ModelCheckpoint('/media/eHD/leticia/models/test13.{epoch:02d}-{val_loss:.2f}.h5', \n",
    "                             monitor='val_loss', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='min', \n",
    "                             period=5)\n",
    "\n",
    "tb_counter  = len([log for log in os.listdir('/media/eHD/leticia/logs/') if 'voc_' in log]) + 1\n",
    "tensorboard = TensorBoard(log_dir=os.path.expanduser('~/logs/') + 'voc_' + '_' + str(tb_counter), \n",
    "                          histogram_freq=0, \n",
    "                          write_graph=True, \n",
    "                          write_images=False)\n",
    " \n",
    "sgd = optimizers.SGD(lr=1e-4, decay=0.0005, momentum=0.9)\n",
    "model.compile(optimizer=sgd, loss=custom_loss) \n",
    "\n",
    "history=model.fit_generator(generator= train_batch, \n",
    "                    steps_per_epoch  = len(train_batch), \n",
    "                    epochs           = 100, \n",
    "                    verbose          = 1,\n",
    "                    validation_data  = valid_batch,\n",
    "                    validation_steps = len(valid_batch),\n",
    "                    callbacks        = [checkpoint, tensorboard, reduce_lr])\n",
    "\n",
    "model.save('/media/eHD/leticia/models/test14.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test14: noobj_scale=0,5\n",
    "plt.plot(history.history['loss']) \n",
    "plt.plot(history.history['val_loss']) \n",
    "plt.title('model loss') \n",
    "plt.ylabel('loss') \n",
    "plt.xlabel('epoch') \n",
    "plt.legend(['train', 'val'], loc='upper left') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "WARM_UP_BATCHES  = 0\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                              patience=5, min_lr=1e-7)\n",
    "\n",
    "checkpoint = ModelCheckpoint('/media/eHD/leticia/models/test14.{epoch:02d}-{val_loss:.2f}.h5', \n",
    "                             monitor='val_loss', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='min', \n",
    "                             period=5)\n",
    "\n",
    "tb_counter  = len([log for log in os.listdir('/media/eHD/leticia/logs/') if 'voc_' in log]) + 1\n",
    "tensorboard = TensorBoard(log_dir=os.path.expanduser('~/logs/') + 'voc_' + '_' + str(tb_counter), \n",
    "                          histogram_freq=0, \n",
    "                          write_graph=True, \n",
    "                          write_images=False)\n",
    " \n",
    "sgd = optimizers.SGD(lr=1e-5, decay=0.0005, momentum=0.9)\n",
    "model.compile(optimizer=sgd, loss=custom_loss) \n",
    "\n",
    "history=model.fit_generator(generator= train_batch, \n",
    "                    steps_per_epoch  = len(train_batch), \n",
    "                    epochs           = 100, \n",
    "                    verbose          = 1,\n",
    "                    validation_data  = valid_batch,\n",
    "                    validation_steps = len(valid_batch),\n",
    "                    callbacks        = [checkpoint, tensorboard, reduce_lr])\n",
    "\n",
    "model.save('/media/eHD/leticia/models/test15.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test15\n",
    "plt.plot(history.history['loss']) \n",
    "plt.plot(history.history['val_loss']) \n",
    "plt.title('model loss') \n",
    "plt.ylabel('loss') \n",
    "plt.xlabel('epoch') \n",
    "plt.legend(['train', 'val'], loc='upper left') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('/media/eHD/leticia/models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "source = os.listdir(\"/home/letica/\")\n",
    "destination = \"/media/eHD/leticia/models/\"\n",
    "for files in source:\n",
    "    if files.endswith(\".h5\"):\n",
    "        shutil.move(files,destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading a trained model:\n",
    "from keras.models import load_model\n",
    "from keras.utils.generic_utils import CustomObjectScope\n",
    "\n",
    "def relu6(x):\n",
    "    return K.relu(x, max_value=6)\n",
    "\n",
    "true_boxes  = Input(shape=(1, 1, 1, TRUE_BOX_BUFFER , 4))\n",
    "\n",
    "with CustomObjectScope({'relu6':relu6, 'custom_loss':custom_loss}):\n",
    "    model = load_model('/media/eHD/leticia/models/test10.h5')\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights=model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4952\n",
      "207\n"
     ]
    }
   ],
   "source": [
    "#Parsing annotations to construct test generator:\n",
    "test_path = '/media/eHD/datasets/pascalVOC2007/VOC_test/'\n",
    "test_ids = get_ids(test_path, test_set)\n",
    "test_imgs, seen_test_labels = parse_annotation(test_path, \n",
    "                                               '/media/eHD/datasets/pascalVOC2007/VOC_test/JPEGImages/', \n",
    "                                                test_ids, labels=LABELS)\n",
    "\n",
    "test_batch = BatchGenerator(test_imgs, generator_config, norm=normalize, jitter=False)\n",
    "\n",
    "print(len(test_imgs))\n",
    "print(len(test_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating the model on test dataset:\n",
    "results = model.evaluate_generator(test_batch, \n",
    "                                   steps=len(test_batch)) \n",
    "\n",
    "print('Final test loss:', (results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute average precision:\n",
    "average_precisions = evaluate(test_batch)     \n",
    "\n",
    "for label, average_precision in average_precisions.items():\n",
    "    print(LABELS[label], '{:.4f}'.format(average_precision))\n",
    "print('mAP: {:.4f}'.format(sum(average_precisions.values()) / len(average_precisions)))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing detection on an image:\n",
    "image = cv2.imread('/media/eHD/datasets/pascalVOC2012/VOCtrain/VOC2012/JPEGImages/2009_003369.jpg')\n",
    "dummy_array = np.zeros((1,1,1,1,TRUE_BOX_BUFFER,4))\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "input_image = cv2.resize(image, (416, 416))\n",
    "input_image = input_image / 255.\n",
    "input_image = input_image[:,:,::-1]\n",
    "input_image = np.expand_dims(input_image, 0)\n",
    "\n",
    "time_start = time.clock()\n",
    "netout = model.predict([input_image, dummy_array])\n",
    "time_elapsed = (time.clock() - time_start)\n",
    "print('Time in ms:', time_elapsed*1000 )\n",
    "\n",
    "boxes = decode_netout(netout[0], \n",
    "                      obj_threshold=0.0255,\n",
    "                      nms_threshold=NMS_THRESHOLD,\n",
    "                      anchors=ANCHORS, \n",
    "                      nb_class=CLASS)\n",
    "            \n",
    "image = draw_boxes(image, boxes, labels=LABELS)\n",
    "\n",
    "plt.imshow(image[:,:,::-1]); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets_from_2007 = [('2007', 'train'), ('2007', 'val')]\n",
    "train_set = [('2012', 'train')]\n",
    "val_set = [('2012', 'val')]\n",
    "test_set = [('2007', 'test')]\n",
    "\n",
    "def get_ids(voc_path, datasets):\n",
    "    \"\"\"Get image identifiers for corresponding list of dataset identifies.\n",
    "    code originally from https://github.com/allanzelener/YAD2K\n",
    "    Parameters\n",
    "    ----------\n",
    "    voc_path : str\n",
    "        Path to VOCdevkit directory.\n",
    "    datasets : list of str tuples\n",
    "        List of dataset identifiers in the form of (year, dataset) pairs.\n",
    "    Returns\n",
    "    -------\n",
    "    ids : list of str\n",
    "        List of all image identifiers for given datasets.\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    for year, image_set in datasets:\n",
    "        id_file = os.path.join(voc_path, 'ImageSets/Main/{}.txt'.format(image_set))\n",
    "        with open(id_file, 'r') as image_ids:\n",
    "            ids.extend(map(str.strip, image_ids.readlines()))\n",
    "    return ids\n",
    "\n",
    "def parse_annotation(ann_dir, img_dir, ids, labels=[]):\n",
    "    all_imgs = []\n",
    "    seen_labels = {}\n",
    "      \n",
    "    for image_id in sorted(ids):\n",
    "        img = {'object':[]}\n",
    "        \n",
    "        fname = os.path.join(ann_dir, 'Annotations/{}.xml'.format(image_id))\n",
    "        with open(fname) as in_file:\n",
    "            tree = ET.parse(in_file)\n",
    "        root = tree.getroot()\n",
    "                            \n",
    "        for elem in tree.iter():\n",
    "            if 'filename' in elem.tag:\n",
    "                img['filename'] = img_dir + elem.text\n",
    "            if 'width' in elem.tag:\n",
    "                img['width'] = int(elem.text)\n",
    "            if 'height' in elem.tag:\n",
    "                img['height'] = int(elem.text)\n",
    "            if 'object' in elem.tag or 'part' in elem.tag:\n",
    "                obj = {}\n",
    "                \n",
    "                for attr in list(elem):\n",
    "                    if 'name' in attr.tag:\n",
    "                        obj['name'] = attr.text\n",
    "\n",
    "                        if obj['name'] in seen_labels:\n",
    "                            seen_labels[obj['name']] += 1\n",
    "                        else:\n",
    "                            seen_labels[obj['name']] = 1\n",
    "                        \n",
    "                        if len(labels) > 0 and obj['name'] not in labels:\n",
    "                            break\n",
    "                        else:\n",
    "                            img['object'] += [obj]\n",
    "                            \n",
    "                    if 'bndbox' in attr.tag:\n",
    "                        for dim in list(attr):\n",
    "                            if 'xmin' in dim.tag:\n",
    "                                obj['xmin'] = int(round(float(dim.text)))\n",
    "                            if 'ymin' in dim.tag:\n",
    "                                obj['ymin'] = int(round(float(dim.text)))\n",
    "                            if 'xmax' in dim.tag:\n",
    "                                obj['xmax'] = int(round(float(dim.text)))\n",
    "                            if 'ymax' in dim.tag:\n",
    "                                obj['ymax'] = int(round(float(dim.text)))\n",
    "\n",
    "        if len(img['object']) > 0:\n",
    "            all_imgs += [img]\n",
    "                        \n",
    "    return all_imgs, seen_labels\n",
    "\n",
    "class BatchGenerator(Sequence):\n",
    "    def __init__(self, images, \n",
    "                       config, \n",
    "                       shuffle=True, \n",
    "                       jitter=True, \n",
    "                       norm=None):\n",
    "        self.generator = None\n",
    "\n",
    "        self.images = images\n",
    "        self.config = config\n",
    "\n",
    "        self.shuffle = shuffle\n",
    "        self.jitter  = jitter\n",
    "        self.norm    = norm\n",
    "\n",
    "        self.anchors = [BoundBox(0, 0, config['ANCHORS'][2*i], config['ANCHORS'][2*i+1]) for i in range(int(len(config['ANCHORS'])//2))]\n",
    "\n",
    "        ### augmentors by https://github.com/aleju/imgaug\n",
    "        sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "\n",
    "        # Define our sequence of augmentation steps that will be applied to every image\n",
    "        # All augmenters with per_channel=0.5 will sample one value _per image_\n",
    "        # in 50% of all cases. In all other cases they will sample new values\n",
    "        # _per channel_.\n",
    "        self.aug_pipe = iaa.Sequential(\n",
    "            [\n",
    "                # apply the following augmenters to most images\n",
    "                #iaa.Fliplr(0.5), # horizontally flip 50% of all images\n",
    "                #iaa.Flipud(0.2), # vertically flip 20% of all images\n",
    "                #sometimes(iaa.Crop(percent=(0, 0.1))), # crop images by 0-10% of their height/width\n",
    "                sometimes(iaa.Affine(\n",
    "                    #scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)}, # scale images to 80-120% of their size, individually per axis\n",
    "                    #translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)}, # translate by -20 to +20 percent (per axis)\n",
    "                    #rotate=(-5, 5), # rotate by -45 to +45 degrees\n",
    "                    #shear=(-5, 5), # shear by -16 to +16 degrees\n",
    "                    #order=[0, 1], # use nearest neighbour or bilinear interpolation (fast)\n",
    "                    #cval=(0, 255), # if mode is constant, use a cval between 0 and 255\n",
    "                    #mode=ia.ALL # use any of scikit-image's warping modes (see 2nd image from the top for examples)\n",
    "                )),\n",
    "                # execute 0 to 5 of the following (less important) augmenters per image\n",
    "                # don't execute all of them, as that would often be way too strong\n",
    "                iaa.SomeOf((0, 5),\n",
    "                    [\n",
    "                        #sometimes(iaa.Superpixels(p_replace=(0, 1.0), n_segments=(20, 200))), # convert images into their superpixel representation\n",
    "                        iaa.OneOf([\n",
    "                            iaa.GaussianBlur((0, 3.0)), # blur images with a sigma between 0 and 3.0\n",
    "                            iaa.AverageBlur(k=(2, 7)), # blur image using local means with kernel sizes between 2 and 7\n",
    "                            iaa.MedianBlur(k=(3, 11)), # blur image using local medians with kernel sizes between 2 and 7\n",
    "                        ]),\n",
    "                        iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)), # sharpen images\n",
    "                        #iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)), # emboss images\n",
    "                        # search either for all edges or for directed edges\n",
    "                        #sometimes(iaa.OneOf([\n",
    "                        #    iaa.EdgeDetect(alpha=(0, 0.7)),\n",
    "                        #    iaa.DirectedEdgeDetect(alpha=(0, 0.7), direction=(0.0, 1.0)),\n",
    "                        #])),\n",
    "                        iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5), # add gaussian noise to images\n",
    "                        iaa.OneOf([\n",
    "                            iaa.Dropout((0.01, 0.1), per_channel=0.5), # randomly remove up to 10% of the pixels\n",
    "                            #iaa.CoarseDropout((0.03, 0.15), size_percent=(0.02, 0.05), per_channel=0.2),\n",
    "                        ]),\n",
    "                        #iaa.Invert(0.05, per_channel=True), # invert color channels\n",
    "                        iaa.Add((-10, 10), per_channel=0.5), # change brightness of images (by -10 to 10 of original value)\n",
    "                        iaa.Multiply((0.5, 1.5), per_channel=0.5), # change brightness of images (50-150% of original value)\n",
    "                        iaa.ContrastNormalization((0.5, 2.0), per_channel=0.5), # improve or worsen the contrast\n",
    "                        #iaa.Grayscale(alpha=(0.0, 1.0)),\n",
    "                        #sometimes(iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25)), # move pixels locally around (with random strengths)\n",
    "                        #sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.05))) # sometimes move parts of the image around\n",
    "                    ],\n",
    "                    random_order=True\n",
    "                )\n",
    "            ],\n",
    "            random_order=True\n",
    "        )\n",
    "\n",
    "        if shuffle: np.random.shuffle(self.images)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(float(len(self.images))/self.config['BATCH_SIZE']))   \n",
    "\n",
    "    def num_classes(self):\n",
    "        return len(self.config['LABELS'])\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.images)    \n",
    "\n",
    "    def load_annotation(self, i):\n",
    "        annots = []\n",
    "\n",
    "        for obj in self.images[i]['object']:\n",
    "            annot = [obj['xmin'], obj['ymin'], obj['xmax'], obj['ymax'], self.config['LABELS'].index(obj['name'])]\n",
    "            annots += [annot]\n",
    "\n",
    "        if len(annots) == 0: annots = [[]]\n",
    "\n",
    "        return np.array(annots)\n",
    "\n",
    "    def load_image(self, i):\n",
    "        return cv2.imread(self.images[i]['filename'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        l_bound = idx*self.config['BATCH_SIZE']\n",
    "        r_bound = (idx+1)*self.config['BATCH_SIZE']\n",
    "\n",
    "        if r_bound > len(self.images):\n",
    "            r_bound = len(self.images)\n",
    "            l_bound = r_bound - self.config['BATCH_SIZE']\n",
    "\n",
    "        instance_count = 0\n",
    "\n",
    "        x_batch = np.zeros((r_bound - l_bound, self.config['IMAGE_H'], self.config['IMAGE_W'], 3))  # input images\n",
    "        b_batch = np.zeros((r_bound - l_bound, 1     , 1     , 1    ,  self.config['TRUE_BOX_BUFFER'], 4))   # list of self.config['TRUE_self.config['BOX']_BUFFER'] GT boxes\n",
    "        y_batch = np.zeros((r_bound - l_bound, self.config['GRID_H'],  self.config['GRID_W'], self.config['BOX'], 4+1+len(self.config['LABELS'])))                # desired network output\n",
    "\n",
    "        for train_instance in self.images[l_bound:r_bound]:\n",
    "            # augment input image and fix object's position and size\n",
    "            img, all_objs = self.aug_image(train_instance, jitter=self.jitter)\n",
    "            \n",
    "            # construct output from object's x, y, w, h\n",
    "            true_box_index = 0\n",
    "            \n",
    "            for obj in all_objs:\n",
    "                if obj['xmax'] > obj['xmin'] and obj['ymax'] > obj['ymin'] and obj['name'] in self.config['LABELS']:\n",
    "                    center_x = .5*(obj['xmin'] + obj['xmax'])\n",
    "                    center_x = center_x / (float(self.config['IMAGE_W']) / self.config['GRID_W'])\n",
    "                    center_y = .5*(obj['ymin'] + obj['ymax'])\n",
    "                    center_y = center_y / (float(self.config['IMAGE_H']) / self.config['GRID_H'])\n",
    "\n",
    "                    grid_x = int(np.floor(center_x))\n",
    "                    grid_y = int(np.floor(center_y))\n",
    "\n",
    "                    if grid_x < self.config['GRID_W'] and grid_y < self.config['GRID_H']:\n",
    "                        obj_indx  = self.config['LABELS'].index(obj['name'])\n",
    "                        \n",
    "                        center_w = (obj['xmax'] - obj['xmin']) / (float(self.config['IMAGE_W']) / self.config['GRID_W']) # unit: grid cell\n",
    "                        center_h = (obj['ymax'] - obj['ymin']) / (float(self.config['IMAGE_H']) / self.config['GRID_H']) # unit: grid cell\n",
    "                        \n",
    "                        box = [center_x, center_y, center_w, center_h]\n",
    "\n",
    "                        # find the anchor that best predicts this box\n",
    "                        best_anchor = -1\n",
    "                        max_iou     = -1\n",
    "                        \n",
    "                        shifted_box = BoundBox(0, \n",
    "                                               0,\n",
    "                                               center_w,                                                \n",
    "                                               center_h)\n",
    "                        \n",
    "                        for i in range(len(self.anchors)):\n",
    "                            anchor = self.anchors[i]\n",
    "                            iou    = bbox_iou(shifted_box, anchor)\n",
    "                            \n",
    "                            if max_iou < iou:\n",
    "                                best_anchor = i\n",
    "                                max_iou     = iou\n",
    "                                \n",
    "                        # assign ground truth x, y, w, h, confidence and class probs to y_batch\n",
    "                        y_batch[instance_count, grid_y, grid_x, best_anchor, 0:4] = box\n",
    "                        y_batch[instance_count, grid_y, grid_x, best_anchor, 4  ] = 1.\n",
    "                        y_batch[instance_count, grid_y, grid_x, best_anchor, 5+obj_indx] = 1\n",
    "                        \n",
    "                        # assign the true box to b_batch\n",
    "                        b_batch[instance_count, 0, 0, 0, true_box_index] = box\n",
    "                        \n",
    "                        true_box_index += 1\n",
    "                        true_box_index = true_box_index % self.config['TRUE_BOX_BUFFER']\n",
    "                            \n",
    "            # assign input image to x_batch\n",
    "            if self.norm != None: \n",
    "                x_batch[instance_count] = self.norm(img)\n",
    "            else:\n",
    "                # plot image and bounding boxes for sanity check\n",
    "                for obj in all_objs:\n",
    "                    if obj['xmax'] > obj['xmin'] and obj['ymax'] > obj['ymin']:\n",
    "                        cv2.rectangle(img[:,:,::-1], (obj['xmin'],obj['ymin']), (obj['xmax'],obj['ymax']), (255,0,0), 3)\n",
    "                        cv2.putText(img[:,:,::-1], obj['name'], \n",
    "                                    (obj['xmin']+2, obj['ymin']+12), \n",
    "                                    0, 1.2e-3 * img.shape[0], \n",
    "                                    (0,255,0), 2)\n",
    "                        \n",
    "                x_batch[instance_count] = img\n",
    "\n",
    "            # increase instance counter in current batch\n",
    "            instance_count += 1  \n",
    "\n",
    "        #print(' new batch created', idx)\n",
    "\n",
    "        return [x_batch, b_batch], y_batch\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle: np.random.shuffle(self.images)\n",
    "\n",
    "    def aug_image(self, train_instance, jitter):\n",
    "        image_name = train_instance['filename']\n",
    "        image = cv2.imread(image_name)\n",
    "\n",
    "        if image is None: print('Cannot find ', image_name)\n",
    "\n",
    "        h, w, c = image.shape\n",
    "        all_objs = copy.deepcopy(train_instance['object'])\n",
    "\n",
    "        if jitter:\n",
    "            ### scale the image\n",
    "            scale = np.random.uniform() / 10. + 1.\n",
    "            image = cv2.resize(image, (0,0), fx = scale, fy = scale)\n",
    "\n",
    "            ### translate the image\n",
    "            max_offx = (scale-1.) * w\n",
    "            max_offy = (scale-1.) * h\n",
    "            offx = int(np.random.uniform() * max_offx)\n",
    "            offy = int(np.random.uniform() * max_offy)\n",
    "            \n",
    "            image = image[offy : (offy + h), offx : (offx + w)]\n",
    "\n",
    "            ### flip the image\n",
    "            flip = np.random.binomial(1, .5)\n",
    "            if flip > 0.5: image = cv2.flip(image, 1)\n",
    "                \n",
    "            image = self.aug_pipe.augment_image(image)            \n",
    "            \n",
    "        # resize the image to standard size\n",
    "        image = cv2.resize(image, (self.config['IMAGE_H'], self.config['IMAGE_W']))\n",
    "        image = image[:,:,::-1]\n",
    "\n",
    "        # fix object's position and size\n",
    "        for obj in all_objs:\n",
    "            for attr in ['xmin', 'xmax']:\n",
    "                if jitter: obj[attr] = int(obj[attr] * scale - offx)\n",
    "                    \n",
    "                obj[attr] = int(obj[attr] * float(self.config['IMAGE_W']) / w)\n",
    "                obj[attr] = max(min(obj[attr], self.config['IMAGE_W']), 0)\n",
    "                \n",
    "            for attr in ['ymin', 'ymax']:\n",
    "                if jitter: obj[attr] = int(obj[attr] * scale - offy)\n",
    "                    \n",
    "                obj[attr] = int(obj[attr] * float(self.config['IMAGE_H']) / h)\n",
    "                obj[attr] = max(min(obj[attr], self.config['IMAGE_H']), 0)\n",
    "\n",
    "            if jitter and flip > 0.5:\n",
    "                xmin = obj['xmin']\n",
    "                obj['xmin'] = self.config['IMAGE_W'] - obj['xmax']\n",
    "                obj['xmax'] = self.config['IMAGE_W'] - xmin\n",
    "                \n",
    "        return image, all_objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundBox:\n",
    "    def __init__(self, xmin, ymin, xmax, ymax, c = None, classes = None):\n",
    "        self.xmin = xmin\n",
    "        self.ymin = ymin\n",
    "        self.xmax = xmax\n",
    "        self.ymax = ymax\n",
    "        \n",
    "        self.c     = c\n",
    "        self.classes = classes\n",
    "\n",
    "        self.label = -1\n",
    "        self.score = -1\n",
    "\n",
    "    def get_label(self):\n",
    "        if self.label == -1:\n",
    "            self.label = np.argmax(self.classes)\n",
    "        \n",
    "        return self.label\n",
    "    \n",
    "    def get_score(self):\n",
    "        if self.score == -1:\n",
    "            self.score = self.classes[self.get_label()]\n",
    "            \n",
    "        return self.score\n",
    "\n",
    "class WeightReader:\n",
    "    def __init__(self, weight_file):\n",
    "        self.offset = 4\n",
    "        self.all_weights = np.fromfile(weight_file, dtype='float32')\n",
    "        \n",
    "    def read_bytes(self, size):\n",
    "        self.offset = self.offset + size\n",
    "        return self.all_weights[self.offset-size:self.offset]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.offset = 4\n",
    "\n",
    "def bbox_iou(box1, box2):\n",
    "    intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n",
    "    intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])  \n",
    "    \n",
    "    intersect = intersect_w * intersect_h\n",
    "\n",
    "    w1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin\n",
    "    w2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin\n",
    "    \n",
    "    union = w1*h1 + w2*h2 - intersect\n",
    "    \n",
    "    return float(intersect) / union\n",
    "\n",
    "def draw_boxes(image, boxes, labels):\n",
    "    image_h, image_w, _ = image.shape\n",
    "\n",
    "    for box in boxes:\n",
    "        xmin = int(box.xmin*image_w)\n",
    "        ymin = int(box.ymin*image_h)\n",
    "        xmax = int(box.xmax*image_w)\n",
    "        ymax = int(box.ymax*image_h)\n",
    "\n",
    "        cv2.rectangle(image, (xmin,ymin), (xmax,ymax), (0,255,0), 3)\n",
    "        print(labels[box.get_label()])\n",
    "        cv2.putText(image, \n",
    "                    labels[box.get_label()] + ' ' + str(box.get_score()), \n",
    "                    (xmin, ymin - 13), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                    1e-3 * image_h, \n",
    "                    (0,255,0), 2)\n",
    "        \n",
    "    return image          \n",
    "        \n",
    "def decode_netout(netout, anchors, nb_class, obj_threshold=0.3, nms_threshold=0.3):\n",
    "    grid_h, grid_w, nb_box = netout.shape[:3]\n",
    "\n",
    "    boxes = []\n",
    "    \n",
    "    # decode the output by the network\n",
    "    netout[..., 4]  = _sigmoid(netout[..., 4])\n",
    "    netout[..., 5:] = netout[..., 4][..., np.newaxis] * _softmax(netout[..., 5:])\n",
    "    print(netout[..., 5:])\n",
    "    netout[..., 5:] *= netout[..., 5:] > obj_threshold\n",
    "    \n",
    "    for row in range(grid_h):\n",
    "        for col in range(grid_w):\n",
    "            for b in range(nb_box):\n",
    "                # from 4th element onwards are confidence and class classes\n",
    "                classes = netout[row,col,b,5:]\n",
    "                \n",
    "                if np.sum(classes) > 0:\n",
    "                    # first 4 elements are x, y, w, and h\n",
    "                    x, y, w, h = netout[row,col,b,:4]\n",
    "\n",
    "                    x = (col + _sigmoid(x)) / grid_w # center position, unit: image width\n",
    "                    y = (row + _sigmoid(y)) / grid_h # center position, unit: image height\n",
    "                    w = anchors[2 * b + 0] * np.exp(w) / grid_w # unit: image width\n",
    "                    h = anchors[2 * b + 1] * np.exp(h) / grid_h # unit: image height\n",
    "                    confidence = netout[row,col,b,4]\n",
    "                    \n",
    "                    box = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, confidence, classes)\n",
    "                    \n",
    "                    boxes.append(box)\n",
    "\n",
    "    # suppress non-maximal boxes\n",
    "    for c in range(nb_class):\n",
    "        sorted_indices = list(reversed(np.argsort([box.classes[c] for box in boxes])))\n",
    "\n",
    "        for i in range(len(sorted_indices)):\n",
    "            index_i = sorted_indices[i]\n",
    "            \n",
    "            if boxes[index_i].classes[c] == 0: \n",
    "                continue\n",
    "            else:\n",
    "                for j in range(i+1, len(sorted_indices)):\n",
    "                    index_j = sorted_indices[j]\n",
    "                    \n",
    "                    if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_threshold:\n",
    "                        boxes[index_j].classes[c] = 0\n",
    "                        \n",
    "    # remove the boxes which are less likely than a obj_threshold\n",
    "    boxes = [box for box in boxes if box.get_score() > obj_threshold]\n",
    "    \n",
    "    return boxes    \n",
    "\n",
    "def compute_overlap(a, b):\n",
    "    \"\"\"\n",
    "    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n",
    "    Parameters\n",
    "    ----------\n",
    "    a: (N, 4) ndarray of float\n",
    "    b: (K, 4) ndarray of float\n",
    "    Returns\n",
    "    -------\n",
    "    overlaps: (N, K) ndarray of overlap between boxes and query_boxes\n",
    "    \"\"\"\n",
    "    area = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n",
    "\n",
    "    iw = np.minimum(np.expand_dims(a[:, 2], axis=1), b[:, 2]) - np.maximum(np.expand_dims(a[:, 0], 1), b[:, 0])\n",
    "    ih = np.minimum(np.expand_dims(a[:, 3], axis=1), b[:, 3]) - np.maximum(np.expand_dims(a[:, 1], 1), b[:, 1])\n",
    "\n",
    "    iw = np.maximum(iw, 0)\n",
    "    ih = np.maximum(ih, 0)\n",
    "\n",
    "    ua = np.expand_dims((a[:, 2] - a[:, 0]) * (a[:, 3] - a[:, 1]), axis=1) + area - iw * ih\n",
    "\n",
    "    ua = np.maximum(ua, np.finfo(float).eps)\n",
    "\n",
    "    intersection = iw * ih\n",
    "\n",
    "    return intersection / ua  \n",
    "    \n",
    "def compute_ap(recall, precision):\n",
    "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
    "    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n",
    "    # Arguments\n",
    "        recall:    The recall curve (list).\n",
    "        precision: The precision curve (list).\n",
    "    # Returns\n",
    "        The average precision as computed in py-faster-rcnn.\n",
    "    \"\"\"\n",
    "    # correct AP calculation\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.], recall, [1.]))\n",
    "    mpre = np.concatenate(([0.], precision, [0.]))\n",
    "\n",
    "    # compute the precision envelope\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # to calculate area under PR curve, look for points\n",
    "    # where X axis (recall) changes value\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap      \n",
    "        \n",
    "def _interval_overlap(interval_a, interval_b):\n",
    "    x1, x2 = interval_a\n",
    "    x3, x4 = interval_b\n",
    "\n",
    "    if x3 < x1:\n",
    "        if x4 < x1:\n",
    "            return 0\n",
    "        else:\n",
    "            return min(x2,x4) - x1\n",
    "    else:\n",
    "        if x2 < x3:\n",
    "             return 0\n",
    "        else:\n",
    "            return min(x2,x4) - x3          \n",
    "\n",
    "def _sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "def _softmax(x, axis=-1, t=-100.):\n",
    "    x = x - np.max(x)\n",
    "    \n",
    "    if np.min(x) < t:\n",
    "        x = x/np.min(x)*t\n",
    "        \n",
    "    e_x = np.exp(x)\n",
    "    \n",
    "    return e_x / e_x.sum(axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def evaluate(generator, \n",
    "                 iou_threshold=0.3,\n",
    "                 score_threshold=0.3,\n",
    "                 max_detections=100,\n",
    "                 save_path=None):\n",
    "        \"\"\" Evaluate a given dataset using a given model.\n",
    "        code originally from https://github.com/fizyr/keras-retinanet\n",
    "        # Arguments\n",
    "            generator       : The generator that represents the dataset to evaluate.\n",
    "            model           : The model to evaluate.\n",
    "            iou_threshold   : The threshold used to consider when a detection is positive or negative.\n",
    "            score_threshold : The score confidence threshold to use for detections.\n",
    "            max_detections  : The maximum number of detections to use per image.\n",
    "            save_path       : The path to save images with visualized detections to.\n",
    "        # Returns\n",
    "            A dict mapping class names to mAP scores.\n",
    "        \"\"\"    \n",
    "        # gather all detections and annotations\n",
    "        all_detections     = [[None for i in range(generator.num_classes())] for j in range(generator.size())]\n",
    "        all_annotations    = [[None for i in range(generator.num_classes())] for j in range(generator.size())]\n",
    "\n",
    "        for i in range(generator.size()):\n",
    "            raw_image = generator.load_image(i)\n",
    "            raw_height, raw_width, raw_channels = raw_image.shape\n",
    "\n",
    "            # make the boxes and the labels\n",
    "            pred_boxes  = predict(raw_image)\n",
    "\n",
    "            score = np.array([box.score for box in pred_boxes])\n",
    "            pred_labels = np.array([box.label for box in pred_boxes])        \n",
    "            \n",
    "            if len(pred_boxes) > 0:\n",
    "                pred_boxes = np.array([[box.xmin*raw_width, box.ymin*raw_height, box.xmax*raw_width, box.ymax*raw_height, box.score] for box in pred_boxes])\n",
    "            else:\n",
    "                pred_boxes = np.array([[]])  \n",
    "            \n",
    "            # sort the boxes and the labels according to scores\n",
    "            score_sort = np.argsort(-score)\n",
    "            pred_labels = pred_labels[score_sort]\n",
    "            pred_boxes  = pred_boxes[score_sort]\n",
    "            \n",
    "            # copy detections to all_detections\n",
    "            for label in range(generator.num_classes()):\n",
    "                all_detections[i][label] = pred_boxes[pred_labels == label, :]\n",
    "                \n",
    "            annotations = generator.load_annotation(i)\n",
    "            \n",
    "            # copy detections to all_annotations\n",
    "            for label in range(generator.num_classes()):\n",
    "                all_annotations[i][label] = annotations[annotations[:, 4] == label, :4].copy()\n",
    "                \n",
    "        # compute mAP by comparing all detections and all annotations\n",
    "        average_precisions = {}\n",
    "        \n",
    "        for label in range(generator.num_classes()):\n",
    "            false_positives = np.zeros((0,))\n",
    "            true_positives  = np.zeros((0,))\n",
    "            scores          = np.zeros((0,))\n",
    "            num_annotations = 0.0\n",
    "\n",
    "            for i in range(generator.size()):\n",
    "                detections           = all_detections[i][label]\n",
    "                annotations          = all_annotations[i][label]\n",
    "                num_annotations     += annotations.shape[0]\n",
    "                detected_annotations = []\n",
    "\n",
    "                for d in detections:\n",
    "                    scores = np.append(scores, d[4])\n",
    "\n",
    "                    if annotations.shape[0] == 0:\n",
    "                        false_positives = np.append(false_positives, 1)\n",
    "                        true_positives  = np.append(true_positives, 0)\n",
    "                        continue\n",
    "\n",
    "                    overlaps            = compute_overlap(np.expand_dims(d, axis=0), annotations)\n",
    "                    assigned_annotation = np.argmax(overlaps, axis=1)\n",
    "                    max_overlap         = overlaps[0, assigned_annotation]\n",
    "\n",
    "                    if max_overlap >= iou_threshold and assigned_annotation not in detected_annotations:\n",
    "                        false_positives = np.append(false_positives, 0)\n",
    "                        true_positives  = np.append(true_positives, 1)\n",
    "                        detected_annotations.append(assigned_annotation)\n",
    "                    else:\n",
    "                        false_positives = np.append(false_positives, 1)\n",
    "                        true_positives  = np.append(true_positives, 0)\n",
    "\n",
    "            # no annotations -> AP for this class is 0 (is this correct?)\n",
    "            if num_annotations == 0:\n",
    "                average_precisions[label] = 0\n",
    "                continue\n",
    "\n",
    "            # sort by score\n",
    "            indices         = np.argsort(-scores)\n",
    "            false_positives = false_positives[indices]\n",
    "            true_positives  = true_positives[indices]\n",
    "\n",
    "            # compute false positives and true positives\n",
    "            false_positives = np.cumsum(false_positives)\n",
    "            true_positives  = np.cumsum(true_positives)\n",
    "\n",
    "            # compute recall and precision\n",
    "            recall    = true_positives / num_annotations\n",
    "            precision = true_positives / np.maximum(true_positives + false_positives, np.finfo(np.float64).eps)\n",
    "\n",
    "            # compute average precision\n",
    "            average_precision  = compute_ap(recall, precision)  \n",
    "            average_precisions[label] = average_precision\n",
    "\n",
    "        return average_precisions    \n",
    "\n",
    "    def predict(image):\n",
    "        image_h, image_w, _ = image.shape\n",
    "        image = cv2.resize(image, (IMAGE_H, IMAGE_W))\n",
    "        image = normalize(image)\n",
    "\n",
    "        input_image = image[:,:,::-1]\n",
    "        input_image = np.expand_dims(input_image, 0)\n",
    "        dummy_array = np.zeros((1,1,1,1,TRUE_BOX_BUFFER,4))\n",
    "\n",
    "        netout = model.predict([input_image, dummy_array])[0]\n",
    "        boxes  = decode_netout(netout, ANCHORS, CLASS)\n",
    "\n",
    "        return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
