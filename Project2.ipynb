{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOLOv2 lightweight implementation based on: https://github.com/experiencor/keras-yolo2\n",
    "\n",
    "#Imports and GPU configuration:\n",
    "from keras import backend as K\n",
    "from keras import layers \n",
    "from keras import models \n",
    "from keras import optimizers\n",
    "from keras.utils import Sequence\n",
    "from keras.applications.mobilenet import MobileNet  \n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from keras.layers import Conv2D, SeparableConv2D, BatchNormalization, LeakyReLU, Input, Lambda, Reshape\n",
    "\n",
    "import os \n",
    "import cv2\n",
    "import copy\n",
    "import time\n",
    "import h5py\n",
    "import keras\n",
    "import numpy as np\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import matplotlib.pyplot as plt \n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "print(tf.__version__, keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up variables:\n",
    "LABELS = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow',\n",
    "          'diningtable', 'dog', 'horse','motorbike', 'person', 'pottedplant','sheep', 'sofa',\n",
    "          'train', 'tvmonitor']\n",
    "\n",
    "IMAGE_H, IMAGE_W = 416, 416\n",
    "GRID_H,  GRID_W  = 13 , 13\n",
    "BOX              = 5\n",
    "CLASS            = len(LABELS)\n",
    "CLASS_WEIGHTS    = np.ones(CLASS, dtype='float32')\n",
    "OBJ_THRESHOLD    = 0.3#0.5\n",
    "NMS_THRESHOLD    = 0.3#0.45\n",
    "ANCHORS          = [0.57273, 0.677385, 1.87446, 2.06253, 3.33843, 5.47434, 7.88282, 3.52778, 9.77052, 9.16828]\n",
    "\n",
    "NO_OBJECT_SCALE  = 1.0\n",
    "OBJECT_SCALE     = 5.0\n",
    "COORD_SCALE      = 1.0\n",
    "CLASS_SCALE      = 1.0\n",
    "\n",
    "BATCH_SIZE       = 24\n",
    "WARM_UP_BATCHES  = 3 \n",
    "TRUE_BOX_BUFFER  = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting pre-trained convolutional base:\n",
    "conv_base=MobileNet(input_shape=(224,224,3), include_top=False, weights='imagenet')\n",
    "conv_base.load_weights('/media/eHD/leticia/mobilenet_backend.h5')\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.mobilenetv2 import MobileNetV2\n",
    "conv_base=MobileNetV2(input_shape=(224,224,3), include_top=False, weights='imagenet')\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the detection model:\n",
    "input_image = Input(shape=(IMAGE_H, IMAGE_W, 3))\n",
    "true_boxes  = Input(shape=(1, 1, 1, TRUE_BOX_BUFFER , 4))\n",
    "\n",
    "x = conv_base(input_image)\n",
    "\n",
    "x = SeparableConv2D(1024, (3, 3), padding='same', use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "x = SeparableConv2D(1024, (3, 3), padding='same', use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "x = SeparableConv2D(1024, (3, 3), padding='same', use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "x = Conv2D(BOX * (4 + 1 + CLASS), (1,1), padding='same', activation='linear')(x)\n",
    "output = Reshape((GRID_H, GRID_W, BOX, 4 + 1 + CLASS))(x)\n",
    "output = Lambda(lambda args: args[0])([output, true_boxes])\n",
    "\n",
    "conv_base.trainable = True\n",
    "\n",
    "model = models.Model([input_image, true_boxes], output)\n",
    "\n",
    "# Initializing the weights of the detection layer: \n",
    "layer = model.layers[-4]\n",
    "weights = layer.get_weights()\n",
    "\n",
    "new_kernel = np.random.normal(size=weights[0].shape)/(GRID_H*GRID_W)\n",
    "new_bias   = np.random.normal(size=weights[1].shape)/(GRID_H*GRID_W)\n",
    "\n",
    "layer.set_weights([new_kernel, new_bias])\n",
    "        \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining YOLO loss function:\n",
    "def custom_loss(y_true, y_pred):\n",
    "    mask_shape = tf.shape(y_true)[:4]\n",
    "    \n",
    "    cell_x = tf.to_float(tf.reshape(tf.tile(tf.range(GRID_W), [GRID_H]), (1, GRID_H, GRID_W, 1, 1)))\n",
    "    cell_y = tf.transpose(cell_x, (0,2,1,3,4))\n",
    "\n",
    "    cell_grid = tf.tile(tf.concat([cell_x,cell_y], -1), [BATCH_SIZE, 1, 1, 5, 1])\n",
    "    \n",
    "    coord_mask = tf.zeros(mask_shape)\n",
    "    conf_mask  = tf.zeros(mask_shape)\n",
    "    class_mask = tf.zeros(mask_shape)\n",
    "    \n",
    "    seen = tf.Variable(0.)\n",
    "    total_recall = tf.Variable(0.)\n",
    "    \n",
    "    \"\"\"\n",
    "    Adjust prediction\n",
    "    \"\"\"\n",
    "    ### adjust x and y      \n",
    "    pred_box_xy = tf.sigmoid(y_pred[..., :2]) + cell_grid\n",
    "    \n",
    "    ### adjust w and h\n",
    "    pred_box_wh = tf.exp(y_pred[..., 2:4]) * np.reshape(ANCHORS, [1,1,1,BOX,2])\n",
    "    \n",
    "    ### adjust confidence\n",
    "    pred_box_conf = tf.sigmoid(y_pred[..., 4])\n",
    "    \n",
    "    ### adjust class probabilities\n",
    "    pred_box_class = y_pred[..., 5:]\n",
    "    \n",
    "    \"\"\"\n",
    "    Adjust ground truth\n",
    "    \"\"\"\n",
    "    ### adjust x and y\n",
    "    true_box_xy = y_true[..., 0:2] # relative position to the containing cell\n",
    "    \n",
    "    ### adjust w and h\n",
    "    true_box_wh = y_true[..., 2:4] # number of cells accross, horizontally and vertically\n",
    "    \n",
    "    ### adjust confidence\n",
    "    true_wh_half = true_box_wh / 2.\n",
    "    true_mins    = true_box_xy - true_wh_half\n",
    "    true_maxes   = true_box_xy + true_wh_half\n",
    "    \n",
    "    pred_wh_half = pred_box_wh / 2.\n",
    "    pred_mins    = pred_box_xy - pred_wh_half\n",
    "    pred_maxes   = pred_box_xy + pred_wh_half       \n",
    "    \n",
    "    intersect_mins  = tf.maximum(pred_mins,  true_mins)\n",
    "    intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
    "    intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "    \n",
    "    true_areas = true_box_wh[..., 0] * true_box_wh[..., 1]\n",
    "    pred_areas = pred_box_wh[..., 0] * pred_box_wh[..., 1]\n",
    "\n",
    "    union_areas = pred_areas + true_areas - intersect_areas\n",
    "    iou_scores  = tf.truediv(intersect_areas, union_areas)\n",
    "    \n",
    "    true_box_conf = iou_scores * y_true[..., 4]\n",
    "    \n",
    "    ### adjust class probabilities\n",
    "    true_box_class = tf.argmax(y_true[..., 5:], -1)\n",
    "    \n",
    "    \"\"\"\n",
    "    Determine the masks\n",
    "    \"\"\"\n",
    "    ### coordinate mask: simply the position of the ground truth boxes (the predictors)\n",
    "    coord_mask = tf.expand_dims(y_true[..., 4], axis=-1) * COORD_SCALE\n",
    "    \n",
    "    ### confidence mask: penelize predictors + penalize boxes with low IOU\n",
    "    # penalize the confidence of the boxes, which have IOU with some ground truth box < 0.6\n",
    "    true_xy = true_boxes[..., 0:2]\n",
    "    true_wh = true_boxes[..., 2:4]\n",
    "    \n",
    "    true_wh_half = true_wh / 2.\n",
    "    true_mins    = true_xy - true_wh_half\n",
    "    true_maxes   = true_xy + true_wh_half\n",
    "    \n",
    "    pred_xy = tf.expand_dims(pred_box_xy, 4)\n",
    "    pred_wh = tf.expand_dims(pred_box_wh, 4)\n",
    "    \n",
    "    pred_wh_half = pred_wh / 2.\n",
    "    pred_mins    = pred_xy - pred_wh_half\n",
    "    pred_maxes   = pred_xy + pred_wh_half    \n",
    "    \n",
    "    intersect_mins  = tf.maximum(pred_mins,  true_mins)\n",
    "    intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
    "    intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "    \n",
    "    true_areas = true_wh[..., 0] * true_wh[..., 1]\n",
    "    pred_areas = pred_wh[..., 0] * pred_wh[..., 1]\n",
    "\n",
    "    union_areas = pred_areas + true_areas - intersect_areas\n",
    "    iou_scores  = tf.truediv(intersect_areas, union_areas)\n",
    "\n",
    "    best_ious = tf.reduce_max(iou_scores, axis=4)\n",
    "    conf_mask = conf_mask + tf.to_float(best_ious < 0.6) * (1 - y_true[..., 4]) * NO_OBJECT_SCALE\n",
    "    \n",
    "    # penalize the confidence of the boxes, which are reponsible for corresponding ground truth box\n",
    "    conf_mask = conf_mask + y_true[..., 4] * OBJECT_SCALE\n",
    "    \n",
    "    ### class mask: simply the position of the ground truth boxes (the predictors)\n",
    "    class_mask = y_true[..., 4] * tf.gather(CLASS_WEIGHTS, true_box_class) * CLASS_SCALE       \n",
    "    \n",
    "    \"\"\"\n",
    "    Warm-up training\n",
    "    \"\"\"\n",
    "    no_boxes_mask = tf.to_float(coord_mask < COORD_SCALE/2.)\n",
    "    seen = tf.assign_add(seen, 1.)\n",
    "    \n",
    "    true_box_xy, true_box_wh, coord_mask = tf.cond(tf.less(seen, WARM_UP_BATCHES), \n",
    "                          lambda: [true_box_xy + (0.5 + cell_grid) * no_boxes_mask, \n",
    "                                   true_box_wh + tf.ones_like(true_box_wh) * np.reshape(ANCHORS, [1,1,1,BOX,2]) * no_boxes_mask, \n",
    "                                   tf.ones_like(coord_mask)],\n",
    "                          lambda: [true_box_xy, \n",
    "                                   true_box_wh,\n",
    "                                   coord_mask])\n",
    "    \n",
    "    \"\"\"\n",
    "    Finalize the loss\n",
    "    \"\"\"\n",
    "    nb_coord_box = tf.reduce_sum(tf.to_float(coord_mask > 0.0))\n",
    "    nb_conf_box  = tf.reduce_sum(tf.to_float(conf_mask  > 0.0))\n",
    "    nb_class_box = tf.reduce_sum(tf.to_float(class_mask > 0.0))\n",
    "    \n",
    "    loss_xy    = tf.reduce_sum(tf.square(true_box_xy-pred_box_xy)     * coord_mask) / (nb_coord_box + 1e-6) / 2.\n",
    "    loss_wh    = tf.reduce_sum(tf.square(true_box_wh-pred_box_wh)     * coord_mask) / (nb_coord_box + 1e-6) / 2.\n",
    "    loss_conf  = tf.reduce_sum(tf.square(true_box_conf-pred_box_conf) * conf_mask)  / (nb_conf_box  + 1e-6) / 2.\n",
    "    loss_class = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=true_box_class, logits=pred_box_class)\n",
    "    loss_class = tf.reduce_sum(loss_class * class_mask) / (nb_class_box + 1e-6)\n",
    "    \n",
    "    loss = tf.cond(tf.less(seen, WARM_UP_BATCHES), \n",
    "                      lambda: loss_xy + loss_wh + loss_conf + loss_class + 10,\n",
    "                      lambda: loss_xy + loss_wh + loss_conf + loss_class)\n",
    "    \n",
    "    \"\"\"\n",
    "    Debugging code\n",
    "    \"\"\"    \n",
    "    nb_true_box = tf.reduce_sum(y_true[..., 4])\n",
    "    nb_pred_box = tf.reduce_sum(tf.to_float(true_box_conf > 0.5) * tf.to_float(pred_box_conf > 0.3))\n",
    "    \n",
    "    current_recall = nb_pred_box/(nb_true_box + 1e-6)\n",
    "    total_recall = tf.assign_add(total_recall, current_recall) \n",
    "\n",
    "    loss = tf.Print(loss, [tf.zeros((1))], message='Dummy Line \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss_xy], message='Loss XY \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss_wh], message='Loss WH \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss_conf], message='Loss Conf \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss_class], message='Loss Class \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss], message='Total Loss \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [current_recall], message='Current Recall \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [total_recall/seen], message='Average Recall \\t', summarize=1000)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_config = {\n",
    "    'IMAGE_H'         : IMAGE_H, \n",
    "    'IMAGE_W'         : IMAGE_W,\n",
    "    'GRID_H'          : GRID_H,  \n",
    "    'GRID_W'          : GRID_W,\n",
    "    'BOX'             : BOX,\n",
    "    'LABELS'          : LABELS,\n",
    "    'CLASS'           : len(LABELS),\n",
    "    'ANCHORS'         : ANCHORS,\n",
    "    'BATCH_SIZE'      : BATCH_SIZE,\n",
    "    'TRUE_BOX_BUFFER' : 50,\n",
    "}\n",
    "\n",
    "#MobileNet normalization function:\n",
    "def normalize(image):\n",
    "    image = image / 255.\n",
    "    image = image - 0.5\n",
    "    image = image * 2.\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsing annotations to construct train generator and validation generator:\n",
    "sets_from_2007 = [('2007', 'train'), ('2007', 'val')]\n",
    "train_set = [('2012', 'train')]\n",
    "val_set = [('2012', 'val')]\n",
    "\n",
    "voc_path ='/media/eHD/datasets/pascalVOC2012/VOCtrain/VOC2012/'\n",
    "train_ids = get_ids(voc_path, train_set)\n",
    "val_ids = get_ids(voc_path, val_set)\n",
    "train_ids_2007 = get_ids('/media/eHD/datasets/pascalVOC2007/VOC_train/', sets_from_2007)\n",
    "\n",
    "train_imgs_2007, seen_train_labels_2007 = parse_annotation('/media/eHD/datasets/pascalVOC2007/VOC_train/', \n",
    "                                                           '/media/eHD/datasets/pascalVOC2007/VOC_train/JPEGImages/', \n",
    "                                                           train_ids_2007, labels=LABELS)\n",
    "train_imgs_2012, seen_train_labels_2012 = parse_annotation(voc_path, '/media/eHD/datasets/pascalVOC2012/VOCtrain/VOC2012/JPEGImages/', \n",
    "                                                           train_ids, labels=LABELS)\n",
    "train_imgs=train_imgs_2007+train_imgs_2012\n",
    "valid_imgs, seen_valid_labels = parse_annotation(voc_path, '/media/eHD/datasets/pascalVOC2012/VOCtrain/VOC2012/JPEGImages/', \n",
    "                                                 val_ids, labels=LABELS)\n",
    "\n",
    "train_batch = BatchGenerator(train_imgs, generator_config, norm=normalize)\n",
    "valid_batch = BatchGenerator(valid_imgs, generator_config, norm=normalize, jitter=False)\n",
    "\n",
    "print(len(train_imgs_2007))\n",
    "print(len(train_imgs_2012))\n",
    "print(len(train_imgs))\n",
    "print(len(valid_imgs))\n",
    "print(len(train_batch))\n",
    "print(len(valid_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Setting callbacks and training:\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                              factor=0.1,\n",
    "                              patience=5,\n",
    "                              verbose=1,\n",
    "                              mode='min',\n",
    "                              min_lr=1e-7)\n",
    "\n",
    "checkpoint = ModelCheckpoint('/media/eHD/leticia/models/test1.{epoch:02d}-{val_loss:.2f}.h5', \n",
    "                             monitor='val_loss', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='min', \n",
    "                             period=5)\n",
    "\n",
    "tb_counter  = len([log for log in os.listdir('/media/eHD/leticia/logs/') if 'voc_' in log]) + 1\n",
    "tensorboard_cb = TensorBoard(log_dir=os.path.expanduser('~/logs/') + 'voc_' + '_' + str(tb_counter), \n",
    "                          histogram_freq=0, \n",
    "                          write_graph=True, \n",
    "                          write_images=False)\n",
    " \n",
    "root, ext = os.path.splitext('/media/eHD/leticia/models/test1.h5')\n",
    "map_evaluator_cb = MAP_evaluation(self, valid_batch,\n",
    "                                  save_best=True,\n",
    "                                  save_name=root+\"_bestMap\"+ext,\n",
    "                                  tensorboard=tensorboard_cb)\n",
    "\n",
    "callbacks = [reduce_lr, checkpoint, tensorboard_cb, map_evaluator_cb]\n",
    "    \n",
    "sgd = optimizers.SGD(lr=1e-4, decay=0.0005, momentum=0.9)\n",
    "model.compile(optimizer=sgd, loss=custom_loss) \n",
    "\n",
    "history=model.fit_generator(generator= train_batch, \n",
    "                    steps_per_epoch  = len(train_batch), \n",
    "                    epochs           = 100, \n",
    "                    verbose          = 1,\n",
    "                    validation_data  = valid_batch,\n",
    "                    validation_steps = len(valid_batch),\n",
    "                    callbacks        = callbacks)\n",
    "\n",
    "model.save('/media/eHD/leticia/models/test1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training curves\n",
    "plt.plot(history.history['loss']) \n",
    "plt.plot(history.history['val_loss']) \n",
    "plt.title('model loss') \n",
    "plt.ylabel('loss') \n",
    "plt.xlabel('epoch') \n",
    "plt.legend(['train', 'val'], loc='upper left') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading a trained model:\n",
    "from keras.models import load_model\n",
    "from keras.utils.generic_utils import CustomObjectScope\n",
    "\n",
    "def relu6(x):\n",
    "    return K.relu(x, max_value=6)\n",
    "\n",
    "true_boxes  = Input(shape=(1, 1, 1, TRUE_BOX_BUFFER , 4))\n",
    "\n",
    "with CustomObjectScope({'relu6':relu6, 'custom_loss':custom_loss}):\n",
    "    model = load_model('/media/eHD/leticia/models/test1.h5')\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsing annotations to construct test generator:\n",
    "test_set = [('2007', 'test')]\n",
    "test_path = '/media/eHD/datasets/pascalVOC2007/VOC_test/'\n",
    "test_ids = get_ids(test_path, test_set)\n",
    "test_imgs, seen_test_labels = parse_annotation(test_path, \n",
    "                                               '/media/eHD/datasets/pascalVOC2007/VOC_test/JPEGImages/', \n",
    "                                                test_ids, labels=LABELS)\n",
    "\n",
    "test_batch = BatchGenerator(test_imgs, generator_config, norm=normalize, jitter=False)\n",
    "\n",
    "print('Number of test images:', len(test_imgs))\n",
    "print('Length of test batch:', len(test_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating the model on test dataset:\n",
    "results = model.evaluate_generator(test_batch, \n",
    "                                   steps=len(test_batch)) \n",
    "\n",
    "print('Final test loss:', (results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute average precision:\n",
    "average_precisions = evaluate(test_batch)     \n",
    "\n",
    "for label, average_precision in average_precisions.items():\n",
    "    print(LABELS[label], '{:.4f}'.format(average_precision))\n",
    "print('mAP: {:.4f}'.format(sum(average_precisions.values()) / len(average_precisions)))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing detection on an image:\n",
    "image = cv2.imread('/home/letica/keras-yolo2/images/person.jpg')\n",
    "dummy_array = np.zeros((1,1,1,1,TRUE_BOX_BUFFER,4))\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "input_image = cv2.resize(image, (416, 416))\n",
    "input_image = input_image / 255.\n",
    "input_image = input_image - 0.5\n",
    "input_image = input_image * 2\n",
    "input_image = input_image[:,:,::-1]\n",
    "input_image = np.expand_dims(input_image, 0)\n",
    "\n",
    "t=time.time()\n",
    "netout = model.predict([input_image, dummy_array])\n",
    "print(\"FPS: \", 1/(time.time()-t))\n",
    "\n",
    "boxes = decode_netout(netout[0], \n",
    "                      obj_threshold=0.3,\n",
    "                      nms_threshold=NMS_THRESHOLD,\n",
    "                      anchors=ANCHORS, \n",
    "                      nb_class=CLASS)\n",
    "            \n",
    "image = draw_boxes(image, boxes, labels=LABELS)\n",
    "\n",
    "plt.imshow(image[:,:,::-1]); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_flops():\n",
    "    # Print to stdout an analysis of the number of floating point operations in the\n",
    "    # model broken down by individual operations.\n",
    "    tf.profiler.profile(\n",
    "        tf.get_default_graph(),\n",
    "        options=tf.profiler.ProfileOptionBuilder.float_operation(), cmd='scope')\n",
    "    \n",
    "calculate_flops()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing functions\n",
    "def get_ids(voc_path, datasets):\n",
    "    \"\"\"Get image identifiers for corresponding list of dataset identifies.\n",
    "    code originally from https://github.com/allanzelener/YAD2K\n",
    "    Parameters\n",
    "    ----------\n",
    "    voc_path : str\n",
    "        Path to VOCdevkit directory.\n",
    "    datasets : list of str tuples\n",
    "        List of dataset identifiers in the form of (year, dataset) pairs.\n",
    "    Returns\n",
    "    -------\n",
    "    ids : list of str\n",
    "        List of all image identifiers for given datasets.\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    for year, image_set in datasets:\n",
    "        id_file = os.path.join(voc_path, 'ImageSets/Main/{}.txt'.format(image_set))\n",
    "        with open(id_file, 'r') as image_ids:\n",
    "            ids.extend(map(str.strip, image_ids.readlines()))\n",
    "    return ids\n",
    "\n",
    "def parse_annotation(ann_dir, img_dir, ids, labels=[]):\n",
    "    all_imgs = []\n",
    "    seen_labels = {}\n",
    "      \n",
    "    for image_id in sorted(ids):\n",
    "        img = {'object':[]}\n",
    "        \n",
    "        fname = os.path.join(ann_dir, 'Annotations/{}.xml'.format(image_id))\n",
    "        with open(fname) as in_file:\n",
    "            tree = ET.parse(in_file)\n",
    "        root = tree.getroot()\n",
    "                            \n",
    "        for elem in tree.iter():\n",
    "            if 'filename' in elem.tag:\n",
    "                img['filename'] = img_dir + elem.text\n",
    "            if 'width' in elem.tag:\n",
    "                img['width'] = int(elem.text)\n",
    "            if 'height' in elem.tag:\n",
    "                img['height'] = int(elem.text)\n",
    "            if 'object' in elem.tag or 'part' in elem.tag:\n",
    "                obj = {}\n",
    "                \n",
    "                for attr in list(elem):\n",
    "                    if 'name' in attr.tag:\n",
    "                        obj['name'] = attr.text\n",
    "\n",
    "                        if obj['name'] in seen_labels:\n",
    "                            seen_labels[obj['name']] += 1\n",
    "                        else:\n",
    "                            seen_labels[obj['name']] = 1\n",
    "                        \n",
    "                        if len(labels) > 0 and obj['name'] not in labels:\n",
    "                            break\n",
    "                        else:\n",
    "                            img['object'] += [obj]\n",
    "                            \n",
    "                    if 'bndbox' in attr.tag:\n",
    "                        for dim in list(attr):\n",
    "                            if 'xmin' in dim.tag:\n",
    "                                obj['xmin'] = int(round(float(dim.text)))\n",
    "                            if 'ymin' in dim.tag:\n",
    "                                obj['ymin'] = int(round(float(dim.text)))\n",
    "                            if 'xmax' in dim.tag:\n",
    "                                obj['xmax'] = int(round(float(dim.text)))\n",
    "                            if 'ymax' in dim.tag:\n",
    "                                obj['ymax'] = int(round(float(dim.text)))\n",
    "\n",
    "        if len(img['object']) > 0:\n",
    "            all_imgs += [img]\n",
    "                        \n",
    "    return all_imgs, seen_labels\n",
    "\n",
    "class BatchGenerator(Sequence):\n",
    "    def __init__(self, images, \n",
    "                       config, \n",
    "                       shuffle=True, \n",
    "                       jitter=True, \n",
    "                       norm=None):\n",
    "        self.generator = None\n",
    "\n",
    "        self.images = images\n",
    "        self.config = config\n",
    "\n",
    "        self.shuffle = shuffle\n",
    "        self.jitter  = jitter\n",
    "        self.norm    = norm\n",
    "\n",
    "        self.anchors = [BoundBox(0, 0, config['ANCHORS'][2*i], config['ANCHORS'][2*i+1]) for i in range(int(len(config['ANCHORS'])//2))]\n",
    "\n",
    "        ### augmentors by https://github.com/aleju/imgaug\n",
    "        sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "\n",
    "        # Define our sequence of augmentation steps that will be applied to every image\n",
    "        # All augmenters with per_channel=0.5 will sample one value _per image_\n",
    "        # in 50% of all cases. In all other cases they will sample new values\n",
    "        # _per channel_.\n",
    "        self.aug_pipe = iaa.Sequential(\n",
    "            [\n",
    "                # apply the following augmenters to most images\n",
    "                #iaa.Fliplr(0.5), # horizontally flip 50% of all images\n",
    "                #iaa.Flipud(0.2), # vertically flip 20% of all images\n",
    "                #sometimes(iaa.Crop(percent=(0, 0.1))), # crop images by 0-10% of their height/width\n",
    "                sometimes(iaa.Affine(\n",
    "                    #scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)}, # scale images to 80-120% of their size, individually per axis\n",
    "                    #translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)}, # translate by -20 to +20 percent (per axis)\n",
    "                    #rotate=(-5, 5), # rotate by -45 to +45 degrees\n",
    "                    #shear=(-5, 5), # shear by -16 to +16 degrees\n",
    "                    #order=[0, 1], # use nearest neighbour or bilinear interpolation (fast)\n",
    "                    #cval=(0, 255), # if mode is constant, use a cval between 0 and 255\n",
    "                    #mode=ia.ALL # use any of scikit-image's warping modes (see 2nd image from the top for examples)\n",
    "                )),\n",
    "                # execute 0 to 5 of the following (less important) augmenters per image\n",
    "                # don't execute all of them, as that would often be way too strong\n",
    "                iaa.SomeOf((0, 5),\n",
    "                    [\n",
    "                        #sometimes(iaa.Superpixels(p_replace=(0, 1.0), n_segments=(20, 200))), # convert images into their superpixel representation\n",
    "                        iaa.OneOf([\n",
    "                            iaa.GaussianBlur((0, 3.0)), # blur images with a sigma between 0 and 3.0\n",
    "                            iaa.AverageBlur(k=(2, 7)), # blur image using local means with kernel sizes between 2 and 7\n",
    "                            iaa.MedianBlur(k=(3, 11)), # blur image using local medians with kernel sizes between 2 and 7\n",
    "                        ]),\n",
    "                        iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)), # sharpen images\n",
    "                        #iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)), # emboss images\n",
    "                        # search either for all edges or for directed edges\n",
    "                        #sometimes(iaa.OneOf([\n",
    "                        #    iaa.EdgeDetect(alpha=(0, 0.7)),\n",
    "                        #    iaa.DirectedEdgeDetect(alpha=(0, 0.7), direction=(0.0, 1.0)),\n",
    "                        #])),\n",
    "                        iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5), # add gaussian noise to images\n",
    "                        iaa.OneOf([\n",
    "                            iaa.Dropout((0.01, 0.1), per_channel=0.5), # randomly remove up to 10% of the pixels\n",
    "                            #iaa.CoarseDropout((0.03, 0.15), size_percent=(0.02, 0.05), per_channel=0.2),\n",
    "                        ]),\n",
    "                        #iaa.Invert(0.05, per_channel=True), # invert color channels\n",
    "                        iaa.Add((-10, 10), per_channel=0.5), # change brightness of images (by -10 to 10 of original value)\n",
    "                        iaa.Multiply((0.5, 1.5), per_channel=0.5), # change brightness of images (50-150% of original value)\n",
    "                        iaa.ContrastNormalization((0.5, 2.0), per_channel=0.5), # improve or worsen the contrast\n",
    "                        #iaa.Grayscale(alpha=(0.0, 1.0)),\n",
    "                        #sometimes(iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25)), # move pixels locally around (with random strengths)\n",
    "                        #sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.05))) # sometimes move parts of the image around\n",
    "                    ],\n",
    "                    random_order=True\n",
    "                )\n",
    "            ],\n",
    "            random_order=True\n",
    "        )\n",
    "\n",
    "        if shuffle: np.random.shuffle(self.images)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(float(len(self.images))/self.config['BATCH_SIZE']))   \n",
    "\n",
    "    def num_classes(self):\n",
    "        return len(self.config['LABELS'])\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.images)    \n",
    "\n",
    "    def load_annotation(self, i):\n",
    "        annots = []\n",
    "\n",
    "        for obj in self.images[i]['object']:\n",
    "            annot = [obj['xmin'], obj['ymin'], obj['xmax'], obj['ymax'], self.config['LABELS'].index(obj['name'])]\n",
    "            annots += [annot]\n",
    "\n",
    "        if len(annots) == 0: annots = [[]]\n",
    "\n",
    "        return np.array(annots)\n",
    "\n",
    "    def load_image(self, i):\n",
    "        return cv2.imread(self.images[i]['filename'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        l_bound = idx*self.config['BATCH_SIZE']\n",
    "        r_bound = (idx+1)*self.config['BATCH_SIZE']\n",
    "\n",
    "        if r_bound > len(self.images):\n",
    "            r_bound = len(self.images)\n",
    "            l_bound = r_bound - self.config['BATCH_SIZE']\n",
    "\n",
    "        instance_count = 0\n",
    "\n",
    "        x_batch = np.zeros((r_bound - l_bound, self.config['IMAGE_H'], self.config['IMAGE_W'], 3))  # input images\n",
    "        b_batch = np.zeros((r_bound - l_bound, 1     , 1     , 1    ,  self.config['TRUE_BOX_BUFFER'], 4))   # list of self.config['TRUE_self.config['BOX']_BUFFER'] GT boxes\n",
    "        y_batch = np.zeros((r_bound - l_bound, self.config['GRID_H'],  self.config['GRID_W'], self.config['BOX'], 4+1+len(self.config['LABELS'])))                # desired network output\n",
    "\n",
    "        for train_instance in self.images[l_bound:r_bound]:\n",
    "            # augment input image and fix object's position and size\n",
    "            img, all_objs = self.aug_image(train_instance, jitter=self.jitter)\n",
    "            \n",
    "            # construct output from object's x, y, w, h\n",
    "            true_box_index = 0\n",
    "            \n",
    "            for obj in all_objs:\n",
    "                if obj['xmax'] > obj['xmin'] and obj['ymax'] > obj['ymin'] and obj['name'] in self.config['LABELS']:\n",
    "                    center_x = .5*(obj['xmin'] + obj['xmax'])\n",
    "                    center_x = center_x / (float(self.config['IMAGE_W']) / self.config['GRID_W'])\n",
    "                    center_y = .5*(obj['ymin'] + obj['ymax'])\n",
    "                    center_y = center_y / (float(self.config['IMAGE_H']) / self.config['GRID_H'])\n",
    "\n",
    "                    grid_x = int(np.floor(center_x))\n",
    "                    grid_y = int(np.floor(center_y))\n",
    "\n",
    "                    if grid_x < self.config['GRID_W'] and grid_y < self.config['GRID_H']:\n",
    "                        obj_indx  = self.config['LABELS'].index(obj['name'])\n",
    "                        \n",
    "                        center_w = (obj['xmax'] - obj['xmin']) / (float(self.config['IMAGE_W']) / self.config['GRID_W']) # unit: grid cell\n",
    "                        center_h = (obj['ymax'] - obj['ymin']) / (float(self.config['IMAGE_H']) / self.config['GRID_H']) # unit: grid cell\n",
    "                        \n",
    "                        box = [center_x, center_y, center_w, center_h]\n",
    "\n",
    "                        # find the anchor that best predicts this box\n",
    "                        best_anchor = -1\n",
    "                        max_iou     = -1\n",
    "                        \n",
    "                        shifted_box = BoundBox(0, \n",
    "                                               0,\n",
    "                                               center_w,                                                \n",
    "                                               center_h)\n",
    "                        \n",
    "                        for i in range(len(self.anchors)):\n",
    "                            anchor = self.anchors[i]\n",
    "                            iou    = bbox_iou(shifted_box, anchor)\n",
    "                            \n",
    "                            if max_iou < iou:\n",
    "                                best_anchor = i\n",
    "                                max_iou     = iou\n",
    "                                \n",
    "                        # assign ground truth x, y, w, h, confidence and class probs to y_batch\n",
    "                        y_batch[instance_count, grid_y, grid_x, best_anchor, 0:4] = box\n",
    "                        y_batch[instance_count, grid_y, grid_x, best_anchor, 4  ] = 1.\n",
    "                        y_batch[instance_count, grid_y, grid_x, best_anchor, 5+obj_indx] = 1\n",
    "                        \n",
    "                        # assign the true box to b_batch\n",
    "                        b_batch[instance_count, 0, 0, 0, true_box_index] = box\n",
    "                        \n",
    "                        true_box_index += 1\n",
    "                        true_box_index = true_box_index % self.config['TRUE_BOX_BUFFER']\n",
    "                            \n",
    "            # assign input image to x_batch\n",
    "            if self.norm != None: \n",
    "                x_batch[instance_count] = self.norm(img)\n",
    "            else:\n",
    "                # plot image and bounding boxes for sanity check\n",
    "                for obj in all_objs:\n",
    "                    if obj['xmax'] > obj['xmin'] and obj['ymax'] > obj['ymin']:\n",
    "                        cv2.rectangle(img[:,:,::-1], (obj['xmin'],obj['ymin']), (obj['xmax'],obj['ymax']), (255,0,0), 3)\n",
    "                        cv2.putText(img[:,:,::-1], obj['name'], \n",
    "                                    (obj['xmin']+2, obj['ymin']+12), \n",
    "                                    0, 1.2e-3 * img.shape[0], \n",
    "                                    (0,255,0), 2)\n",
    "                        \n",
    "                x_batch[instance_count] = img\n",
    "\n",
    "            # increase instance counter in current batch\n",
    "            instance_count += 1  \n",
    "\n",
    "        #print(' new batch created', idx)\n",
    "\n",
    "        return [x_batch, b_batch], y_batch\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle: np.random.shuffle(self.images)\n",
    "\n",
    "    def aug_image(self, train_instance, jitter):\n",
    "        image_name = train_instance['filename']\n",
    "        image = cv2.imread(image_name)\n",
    "\n",
    "        if image is None: print('Cannot find ', image_name)\n",
    "\n",
    "        h, w, c = image.shape\n",
    "        all_objs = copy.deepcopy(train_instance['object'])\n",
    "\n",
    "        if jitter:\n",
    "            ### scale the image\n",
    "            scale = np.random.uniform() / 10. + 1.\n",
    "            image = cv2.resize(image, (0,0), fx = scale, fy = scale)\n",
    "\n",
    "            ### translate the image\n",
    "            max_offx = (scale-1.) * w\n",
    "            max_offy = (scale-1.) * h\n",
    "            offx = int(np.random.uniform() * max_offx)\n",
    "            offy = int(np.random.uniform() * max_offy)\n",
    "            \n",
    "            image = image[offy : (offy + h), offx : (offx + w)]\n",
    "\n",
    "            ### flip the image\n",
    "            flip = np.random.binomial(1, .5)\n",
    "            if flip > 0.5: image = cv2.flip(image, 1)\n",
    "                \n",
    "            image = self.aug_pipe.augment_image(image)            \n",
    "            \n",
    "        # resize the image to standard size\n",
    "        image = cv2.resize(image, (self.config['IMAGE_H'], self.config['IMAGE_W']))\n",
    "        image = image[:,:,::-1]\n",
    "\n",
    "        # fix object's position and size\n",
    "        for obj in all_objs:\n",
    "            for attr in ['xmin', 'xmax']:\n",
    "                if jitter: obj[attr] = int(obj[attr] * scale - offx)\n",
    "                    \n",
    "                obj[attr] = int(obj[attr] * float(self.config['IMAGE_W']) / w)\n",
    "                obj[attr] = max(min(obj[attr], self.config['IMAGE_W']), 0)\n",
    "                \n",
    "            for attr in ['ymin', 'ymax']:\n",
    "                if jitter: obj[attr] = int(obj[attr] * scale - offy)\n",
    "                    \n",
    "                obj[attr] = int(obj[attr] * float(self.config['IMAGE_H']) / h)\n",
    "                obj[attr] = max(min(obj[attr], self.config['IMAGE_H']), 0)\n",
    "\n",
    "            if jitter and flip > 0.5:\n",
    "                xmin = obj['xmin']\n",
    "                obj['xmin'] = self.config['IMAGE_W'] - obj['xmax']\n",
    "                obj['xmax'] = self.config['IMAGE_W'] - xmin\n",
    "                \n",
    "        return image, all_objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utils and postprocessing functions\n",
    "class BoundBox:\n",
    "    def __init__(self, xmin, ymin, xmax, ymax, c = None, classes = None):\n",
    "        self.xmin = xmin\n",
    "        self.ymin = ymin\n",
    "        self.xmax = xmax\n",
    "        self.ymax = ymax\n",
    "        \n",
    "        self.c     = c\n",
    "        self.classes = classes\n",
    "\n",
    "        self.label = -1\n",
    "        self.score = -1\n",
    "\n",
    "    def get_label(self):\n",
    "        if self.label == -1:\n",
    "            self.label = np.argmax(self.classes)\n",
    "        \n",
    "        return self.label\n",
    "    \n",
    "    def get_score(self):\n",
    "        if self.score == -1:\n",
    "            self.score = self.classes[self.get_label()]\n",
    "            \n",
    "        return self.score\n",
    "\n",
    "class WeightReader:\n",
    "    def __init__(self, weight_file):\n",
    "        self.offset = 4\n",
    "        self.all_weights = np.fromfile(weight_file, dtype='float32')\n",
    "        \n",
    "    def read_bytes(self, size):\n",
    "        self.offset = self.offset + size\n",
    "        return self.all_weights[self.offset-size:self.offset]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.offset = 4\n",
    "\n",
    "def bbox_iou(box1, box2):\n",
    "    intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n",
    "    intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])  \n",
    "    \n",
    "    intersect = intersect_w * intersect_h\n",
    "\n",
    "    w1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin\n",
    "    w2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin\n",
    "    \n",
    "    union = w1*h1 + w2*h2 - intersect\n",
    "    \n",
    "    return float(intersect) / union\n",
    "\n",
    "def draw_boxes(image, boxes, labels):\n",
    "    image_h, image_w, _ = image.shape\n",
    "\n",
    "    for box in boxes:\n",
    "        xmin = int(box.xmin*image_w)\n",
    "        ymin = int(box.ymin*image_h)\n",
    "        xmax = int(box.xmax*image_w)\n",
    "        ymax = int(box.ymax*image_h)\n",
    "\n",
    "        cv2.rectangle(image, (xmin,ymin), (xmax,ymax), (0,255,0), 3)\n",
    "        print(labels[box.get_label()])\n",
    "        cv2.putText(image, \n",
    "                    labels[box.get_label()] + ' ' + str(box.get_score()), \n",
    "                    (xmin, ymin - 13), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                    1e-3 * image_h, \n",
    "                    (0,255,0), 2)\n",
    "        \n",
    "    return image          \n",
    "        \n",
    "def decode_netout(netout, anchors, nb_class, obj_threshold=0.3, nms_threshold=0.3):\n",
    "    grid_h, grid_w, nb_box = netout.shape[:3]\n",
    "\n",
    "    boxes = []\n",
    "    \n",
    "    # decode the output by the network\n",
    "    netout[..., 4]  = _sigmoid(netout[..., 4]) #confidence\n",
    "    netout[..., 5:] = netout[..., 4][..., np.newaxis] * _softmax(netout[..., 5:]) #score=confidence*class_prob\n",
    "    netout[..., 5:] *= netout[..., 5:] > obj_threshold\n",
    "    \n",
    "    for row in range(grid_h):\n",
    "        for col in range(grid_w):\n",
    "            for b in range(nb_box):\n",
    "                # from 4th element onwards are confidence and class classes\n",
    "                classes = netout[row,col,b,5:]\n",
    "                \n",
    "                if np.sum(classes) > 0:\n",
    "                    # first 4 elements are x, y, w, and h\n",
    "                    x, y, w, h = netout[row,col,b,:4]\n",
    "\n",
    "                    x = (col + _sigmoid(x)) / grid_w # center position, unit: image width\n",
    "                    y = (row + _sigmoid(y)) / grid_h # center position, unit: image height\n",
    "                    w = anchors[2 * b + 0] * np.exp(w) / grid_w # unit: image width\n",
    "                    h = anchors[2 * b + 1] * np.exp(h) / grid_h # unit: image height\n",
    "                    confidence = netout[row,col,b,4]\n",
    "                    \n",
    "                    box = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, confidence, classes)\n",
    "                    \n",
    "                    boxes.append(box)\n",
    "\n",
    "    # suppress non-maximal boxes\n",
    "    for c in range(nb_class):\n",
    "        sorted_indices = list(reversed(np.argsort([box.classes[c] for box in boxes])))\n",
    "\n",
    "        for i in range(len(sorted_indices)):\n",
    "            index_i = sorted_indices[i]\n",
    "            \n",
    "            if boxes[index_i].classes[c] == 0: \n",
    "                continue\n",
    "            else:\n",
    "                for j in range(i+1, len(sorted_indices)):\n",
    "                    index_j = sorted_indices[j]\n",
    "                    \n",
    "                    if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_threshold:\n",
    "                        boxes[index_j].classes[c] = 0\n",
    "                        \n",
    "    # remove the boxes which are less likely than a obj_threshold\n",
    "    boxes = [box for box in boxes if box.get_score() > obj_threshold]\n",
    "    \n",
    "    return boxes    \n",
    "\n",
    "def compute_overlap(a, b):\n",
    "    \"\"\"\n",
    "    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n",
    "    Parameters\n",
    "    ----------\n",
    "    a: (N, 4) ndarray of float\n",
    "    b: (K, 4) ndarray of float\n",
    "    Returns\n",
    "    -------\n",
    "    overlaps: (N, K) ndarray of overlap between boxes and query_boxes\n",
    "    \"\"\"\n",
    "    area = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n",
    "\n",
    "    iw = np.minimum(np.expand_dims(a[:, 2], axis=1), b[:, 2]) - np.maximum(np.expand_dims(a[:, 0], 1), b[:, 0])\n",
    "    ih = np.minimum(np.expand_dims(a[:, 3], axis=1), b[:, 3]) - np.maximum(np.expand_dims(a[:, 1], 1), b[:, 1])\n",
    "\n",
    "    iw = np.maximum(iw, 0)\n",
    "    ih = np.maximum(ih, 0)\n",
    "\n",
    "    ua = np.expand_dims((a[:, 2] - a[:, 0]) * (a[:, 3] - a[:, 1]), axis=1) + area - iw * ih\n",
    "\n",
    "    ua = np.maximum(ua, np.finfo(float).eps)\n",
    "\n",
    "    intersection = iw * ih\n",
    "\n",
    "    return intersection / ua  \n",
    "    \n",
    "def compute_ap(recall, precision):\n",
    "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
    "    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n",
    "    # Arguments\n",
    "        recall:    The recall curve (list).\n",
    "        precision: The precision curve (list).\n",
    "    # Returns\n",
    "        The average precision as computed in py-faster-rcnn.\n",
    "    \"\"\"\n",
    "    # correct AP calculation\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.], recall, [1.]))\n",
    "    mpre = np.concatenate(([0.], precision, [0.]))\n",
    "\n",
    "    # compute the precision envelope\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # to calculate area under PR curve, look for points\n",
    "    # where X axis (recall) changes value\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap      \n",
    "        \n",
    "def _interval_overlap(interval_a, interval_b):\n",
    "    x1, x2 = interval_a\n",
    "    x3, x4 = interval_b\n",
    "\n",
    "    if x3 < x1:\n",
    "        if x4 < x1:\n",
    "            return 0\n",
    "        else:\n",
    "            return min(x2,x4) - x1\n",
    "    else:\n",
    "        if x2 < x3:\n",
    "             return 0\n",
    "        else:\n",
    "            return min(x2,x4) - x3          \n",
    "\n",
    "def _sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "def _softmax(x, axis=-1, t=-100.):\n",
    "    x = x - np.max(x)\n",
    "    \n",
    "    if np.min(x) < t:\n",
    "        x = x/np.min(x)*t\n",
    "        \n",
    "    e_x = np.exp(x)\n",
    "    \n",
    "    return e_x / e_x.sum(axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def evaluate(generator, \n",
    "                 iou_threshold=0.3,\n",
    "                 score_threshold=0.3,\n",
    "                 max_detections=100,\n",
    "                 save_path=None):\n",
    "        \"\"\" Evaluate a given dataset using a given model.\n",
    "        code originally from https://github.com/fizyr/keras-retinanet\n",
    "        # Arguments\n",
    "            generator       : The generator that represents the dataset to evaluate.\n",
    "            model           : The model to evaluate.\n",
    "            iou_threshold   : The threshold used to consider when a detection is positive or negative.\n",
    "            score_threshold : The score confidence threshold to use for detections.\n",
    "            max_detections  : The maximum number of detections to use per image.\n",
    "            save_path       : The path to save images with visualized detections to.\n",
    "        # Returns\n",
    "            A dict mapping class names to mAP scores.\n",
    "        \"\"\"    \n",
    "        # gather all detections and annotations\n",
    "        all_detections     = [[None for i in range(generator.num_classes())] for j in range(generator.size())]\n",
    "        all_annotations    = [[None for i in range(generator.num_classes())] for j in range(generator.size())]\n",
    "\n",
    "        for i in range(generator.size()):\n",
    "            raw_image = generator.load_image(i)\n",
    "            raw_height, raw_width, raw_channels = raw_image.shape\n",
    "\n",
    "            # make the boxes and the labels\n",
    "            pred_boxes  = predict(raw_image)\n",
    "\n",
    "            score = np.array([box.score for box in pred_boxes])\n",
    "            pred_labels = np.array([box.label for box in pred_boxes])        \n",
    "            \n",
    "            if len(pred_boxes) > 0:\n",
    "                pred_boxes = np.array([[box.xmin*raw_width, box.ymin*raw_height, box.xmax*raw_width, box.ymax*raw_height, box.score] for box in pred_boxes])\n",
    "            else:\n",
    "                pred_boxes = np.array([[]])  \n",
    "            \n",
    "            # sort the boxes and the labels according to scores\n",
    "            score_sort = np.argsort(-score)\n",
    "            pred_labels = pred_labels[score_sort]\n",
    "            pred_boxes  = pred_boxes[score_sort]\n",
    "            \n",
    "            # copy detections to all_detections\n",
    "            for label in range(generator.num_classes()):\n",
    "                all_detections[i][label] = pred_boxes[pred_labels == label, :]\n",
    "                \n",
    "            annotations = generator.load_annotation(i)\n",
    "            \n",
    "            # copy detections to all_annotations\n",
    "            for label in range(generator.num_classes()):\n",
    "                all_annotations[i][label] = annotations[annotations[:, 4] == label, :4].copy()\n",
    "                \n",
    "        # compute mAP by comparing all detections and all annotations\n",
    "        average_precisions = {}\n",
    "        \n",
    "        for label in range(generator.num_classes()):\n",
    "            false_positives = np.zeros((0,))\n",
    "            true_positives  = np.zeros((0,))\n",
    "            scores          = np.zeros((0,))\n",
    "            num_annotations = 0.0\n",
    "\n",
    "            for i in range(generator.size()):\n",
    "                detections           = all_detections[i][label]\n",
    "                annotations          = all_annotations[i][label]\n",
    "                num_annotations     += annotations.shape[0]\n",
    "                detected_annotations = []\n",
    "\n",
    "                for d in detections:\n",
    "                    scores = np.append(scores, d[4])\n",
    "\n",
    "                    if annotations.shape[0] == 0:\n",
    "                        false_positives = np.append(false_positives, 1)\n",
    "                        true_positives  = np.append(true_positives, 0)\n",
    "                        continue\n",
    "\n",
    "                    overlaps            = compute_overlap(np.expand_dims(d, axis=0), annotations)\n",
    "                    assigned_annotation = np.argmax(overlaps, axis=1)\n",
    "                    max_overlap         = overlaps[0, assigned_annotation]\n",
    "\n",
    "                    if max_overlap >= iou_threshold and assigned_annotation not in detected_annotations:\n",
    "                        false_positives = np.append(false_positives, 0)\n",
    "                        true_positives  = np.append(true_positives, 1)\n",
    "                        detected_annotations.append(assigned_annotation)\n",
    "                    else:\n",
    "                        false_positives = np.append(false_positives, 1)\n",
    "                        true_positives  = np.append(true_positives, 0)\n",
    "\n",
    "            # no annotations -> AP for this class is 0 (is this correct?)\n",
    "            if num_annotations == 0:\n",
    "                average_precisions[label] = 0\n",
    "                continue\n",
    "\n",
    "            # sort by score\n",
    "            indices         = np.argsort(-scores)\n",
    "            false_positives = false_positives[indices]\n",
    "            true_positives  = true_positives[indices]\n",
    "\n",
    "            # compute false positives and true positives\n",
    "            false_positives = np.cumsum(false_positives)\n",
    "            true_positives  = np.cumsum(true_positives)\n",
    "\n",
    "            # compute recall and precision\n",
    "            recall    = true_positives / num_annotations\n",
    "            precision = true_positives / np.maximum(true_positives + false_positives, np.finfo(np.float64).eps)\n",
    "\n",
    "            # compute average precision\n",
    "            average_precision  = compute_ap(recall, precision)  \n",
    "            average_precisions[label] = average_precision\n",
    "\n",
    "        return average_precisions    \n",
    "\n",
    "    def predict(image):\n",
    "        image_h, image_w, _ = image.shape\n",
    "        image = cv2.resize(image, (IMAGE_H, IMAGE_W))\n",
    "        image = normalize(image)\n",
    "\n",
    "        input_image = image[:,:,::-1]\n",
    "        input_image = np.expand_dims(input_image, 0)\n",
    "        dummy_array = np.zeros((1,1,1,1,TRUE_BOX_BUFFER,4))\n",
    "\n",
    "        netout = model.predict([input_image, dummy_array])[0]\n",
    "        boxes  = decode_netout(netout, ANCHORS, CLASS)\n",
    "\n",
    "        return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Callback to show mAP during training on tensorboard\n",
    "    class MAP_evaluation(keras.callbacks.Callback):\n",
    "        \"\"\" code originally from https://github.com/rodrigo2019/keras-yolo2\n",
    "            # Arguments\n",
    "                generator       : The generator that represents the dataset to evaluate.\n",
    "                model           : The model to evaluate.\n",
    "                iou_threshold   : The threshold used to consider when a detection is positive or negative.\n",
    "                score_threshold : The score confidence threshold to use for detections.\n",
    "                save_path       : The path to save images with visualized detections to.\n",
    "            # Returns\n",
    "                A dict mapping class names to mAP scores.\n",
    "        \"\"\"   \n",
    "        def __init__(self,\n",
    "                    model,\n",
    "                    generator, \n",
    "                    iou_threshold=0.3,\n",
    "                    score_threshold=0.3,\n",
    "                    save_path=None,\n",
    "                    period=1,\n",
    "                    save_best=False,\n",
    "                    save_name=None,\n",
    "                    tensorboard=None):\n",
    "            \n",
    "            self.model = model\n",
    "            self.generator = generator\n",
    "            self.iou_threshold = iou_threshold\n",
    "            self.save_path = save_path\n",
    "            self.period = period\n",
    "            self.save_best = save_best\n",
    "            self.save_name = save_name\n",
    "            self.tensorboard = tensorboard\n",
    "\n",
    "            self.bestMap = 0\n",
    "\n",
    "            if not isinstance(self.tensorboard,keras.callbacks.TensorBoard) and self.tensorboard is not None:\n",
    "                raise ValueError(\"Tensorboard object must be a instance from keras.callbacks.TensorBoard\")\n",
    "\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs={}):\n",
    "\n",
    "            if epoch % self.period == 0 and self.period != 0:\n",
    "                average_precisions = self.evaluate()\n",
    "                \n",
    "                for label, average_precision in average_precisions.items():\n",
    "                    print(LABELS[label], '{:.4f}'.format(average_precision))\n",
    "                mAP = sum(average_precisions.values()) / len(average_precisions)\n",
    "                print('mAP: {:.4f}'.format(mAP))\n",
    "                \n",
    "                if self.save_best and self.save_name is not None and mAP > self.bestMap:\n",
    "                    print(\"mAP improved from {} to {}, saving model to {}.\".format(self.bestMap,mAP,self.save_name))\n",
    "                    self.bestMap = mAP\n",
    "                    self.model.save(self.save_name)\n",
    "                else:\n",
    "                    print(\"mAP did not improve from {}.\".format(self.bestMap))\n",
    "\n",
    "                if self.tensorboard is not None and self.tensorboard.writer is not None:\n",
    "                    import tensorflow as tf\n",
    "                    summary = tf.Summary()\n",
    "                    summary_value = summary.value.add()\n",
    "                    summary_value.simple_value = mAP\n",
    "                    summary_value.tag = \"val_mAP\"\n",
    "                    self.tensorboard.writer.add_summary(summary, epoch)\n",
    "\n",
    "        def evaluate(self):\n",
    "             \n",
    "            self.model = self.model\n",
    "            # gather all detections and annotations\n",
    "            all_detections     = [[None for i in range(self.generator.num_classes())] for j in range(self.generator.size())]\n",
    "            all_annotations    = [[None for i in range(self.generator.num_classes())] for j in range(self.generator.size())]\n",
    "\n",
    "            for i in range(self.generator.size()):\n",
    "                raw_image = self.generator.load_image(i)\n",
    "                raw_height, raw_width, raw_channels = raw_image.shape\n",
    "\n",
    "                # make the boxes and the labels\n",
    "                pred_boxes  = predict(raw_image)\n",
    "                \n",
    "                score = np.array([box.score for box in pred_boxes])\n",
    "                pred_labels = np.array([box.label for box in pred_boxes])        \n",
    "                \n",
    "                if len(pred_boxes) > 0:\n",
    "                    pred_boxes = np.array([[box.xmin*raw_width, box.ymin*raw_height, box.xmax*raw_width, box.ymax*raw_height, box.score] for box in pred_boxes])\n",
    "                else:\n",
    "                    pred_boxes = np.array([[]])  \n",
    "                \n",
    "                # sort the boxes and the labels according to scores\n",
    "                score_sort = np.argsort(-score)\n",
    "                pred_labels = pred_labels[score_sort]\n",
    "                pred_boxes  = pred_boxes[score_sort]\n",
    "                \n",
    "                # copy detections to all_detections\n",
    "                for label in range(self.generator.num_classes()):\n",
    "                    all_detections[i][label] = pred_boxes[pred_labels == label, :]\n",
    "                    \n",
    "                annotations = self.generator.load_annotation(i)\n",
    "                \n",
    "                # copy detections to all_annotations\n",
    "                for label in range(self.generator.num_classes()):\n",
    "                    all_annotations[i][label] = annotations[annotations[:, 4] == label, :4].copy()\n",
    "                    \n",
    "            # compute mAP by comparing all detections and all annotations\n",
    "            average_precisions = {}\n",
    "            \n",
    "            for label in range(self.generator.num_classes()):\n",
    "                false_positives = np.zeros((0,))\n",
    "                true_positives  = np.zeros((0,))\n",
    "                scores          = np.zeros((0,))\n",
    "                num_annotations = 0.0\n",
    "\n",
    "                for i in range(self.generator.size()):\n",
    "                    detections           = all_detections[i][label]\n",
    "                    annotations          = all_annotations[i][label]\n",
    "                    num_annotations     += annotations.shape[0]\n",
    "                    detected_annotations = []\n",
    "\n",
    "                    for d in detections:\n",
    "                        scores = np.append(scores, d[4])\n",
    "\n",
    "                        if annotations.shape[0] == 0:\n",
    "                            false_positives = np.append(false_positives, 1)\n",
    "                            true_positives  = np.append(true_positives, 0)\n",
    "                            continue\n",
    "\n",
    "                        overlaps            = compute_overlap(np.expand_dims(d, axis=0), annotations)\n",
    "                        assigned_annotation = np.argmax(overlaps, axis=1)\n",
    "                        max_overlap         = overlaps[0, assigned_annotation]\n",
    "\n",
    "                        if max_overlap >= self.iou_threshold and assigned_annotation not in detected_annotations:\n",
    "                            false_positives = np.append(false_positives, 0)\n",
    "                            true_positives  = np.append(true_positives, 1)\n",
    "                            detected_annotations.append(assigned_annotation)\n",
    "                        else:\n",
    "                            false_positives = np.append(false_positives, 1)\n",
    "                            true_positives  = np.append(true_positives, 0)\n",
    "\n",
    "                # no annotations -> AP for this class is 0 (is this correct?)\n",
    "                if num_annotations == 0:\n",
    "                    average_precisions[label] = 0\n",
    "                    continue\n",
    "\n",
    "                # sort by score\n",
    "                indices         = np.argsort(-scores)\n",
    "                false_positives = false_positives[indices]\n",
    "                true_positives  = true_positives[indices]\n",
    "\n",
    "                # compute false positives and true positives\n",
    "                false_positives = np.cumsum(false_positives)\n",
    "                true_positives  = np.cumsum(true_positives)\n",
    "\n",
    "                # compute recall and precision\n",
    "                recall    = true_positives / num_annotations\n",
    "                precision = true_positives / np.maximum(true_positives + false_positives, np.finfo(np.float64).eps)\n",
    "\n",
    "                # compute average precision\n",
    "                average_precision  = compute_ap(recall, precision)  \n",
    "                average_precisions[label] = average_precision\n",
    "\n",
    "            return average_precisions    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
